\chapter{Software architecture}

\section{Approach}

\par
The development of this demonstration application involves the distinction of three layers of abstraction: user interface, rendering components and the library itself.

\par
The top layer is the User Interface which is obviously application, and eventually device, dependent.
The user interface of this dissertation's demo application is very simple and just allows the user to see the rendered image and choose some rendering components to use, like the integrator, the sampler, the number of threads and samples and choose the scene to render.
Although this layer is useful, this dissertation focuses only on the middle and bottom layers.

\begin{figure}[H]
	\centering
	\caption{Illustration of the developed User Interface.}
	\label{UI.}
	\includegraphics[width=10cm,height=10cm,keepaspectratio,scale=1.0]{UI.png}
\end{figure}

\par
The middle layer provides the rendering components which are abstracted concepts about rendering that use functionalities that the library offers to the programmer.
Some of these rendering components are the camera, the light, the sampler and the integrator.
These rendering components are useful for the programmer since it allows them to use features without having the need to know how these were developed.
And, of course, this facilitates and accelerates the development of new rendering applications.

\par
Lastly, the bottom layer is the library itself, which contains the business logic of basic features in a renderer, that the rendering components use.
These features are basic functionalities of a ray tracer engine like: create vectors and points, create different primitives with different materials, cast rays and intersect rays with the primitives.

\par
It is also important to mention that the demo application was developed in order to show the developed features, the performance achieved in the mobile devices and also to help promote the library.
This demo application is a good way to test it in several Android devices like a smart phone or a tablet.

\begin{figure}[H]
	\centering
	\caption{Illustration of the 3 layers in the application}
	\label{Illustration of the 3 layers in the application}
	\includegraphics[width=10cm,height=10cm,keepaspectratio,scale=1.0]{Layers.png}
\end{figure}

\par
Besides the abstraction layers, there are some important strategic decisions made in order to guide the progress of the development of this library.

\par
The first decision was: the primary rays always have origin in the camera.
This decision was made in order to not mix the code of the integrator with the ray tracer renderer engine.

\par
Other decision made was to make the rendering process progressive, which means that the rendered image is incrementally refined with more and more traced rays.
As the integrator will be converging to better values.
This is important in order to give the user a fast rendered image, with some noise or aliasing, and converge it progressively to a better solution with higher details and practically without any visual noise or aliasing.

\par
Another thought aspect that was studied is the permission of dynamic scenes and / or dynamic cameras, which means to let the programmer modify the camera or the scene while the ray tracer is rendering it.
This makes possible to build challenging applications and also provide more interesting scenes and more eye candy applications for the final user.

\par
Last, but not least, is that the code was developed in a modular way, in this case was programmed in an object oriented manner.
This allows the programmers to code their own rendering components, like the integrator, camera, sampler and light, without having to develop the basic features in the renderer engine.

\section{Methodology}

\par
In order to take advantage of most of the mobile CPU resources and give a good performance for the applications, the library and the rendering components were developed using the native programming language C++.
This was achieved by using the Native Development Kit (NDK) provided by the Integrated Development Environment (IDE) Android Studio.
The User Interface was developed in Java using the traditional Software Development Kit (SDK) provided by the Android Studio because there is no framework in the NDK that helps the programmer design his own user interface.
Despite that, its performance is not very important because it doesn't interfere significantly with the others layers of the application.



\iffalse

\section{Work plan}

\par
The development of this dissertation will be done in six stages.

\par
The first stage involves the survey of state of the art about ray tracers built for mobile systems, in order to understand the capacities and limitations of these devices when running rendering algorithms.

\par
The second stage is to make the requirements specification in order to:

\begin{itemize}
\item Describe the functionalities of the library.
\item Explain how the user interact with the external interfaces.
\item Demonstrate the design constraints imposed on an implementation of a custom rendering component.
\end{itemize}

\par
The third stage will address the development of the architecture of the entire system.
The study performed in the previous stages will serve as the basis for this development, where this architecture should be able to address all the possible problems that might arise in the development process.

\par
The fourth stage will be the development and test of the software, where it should be relatively quick if the previous stage was done properly.

\par
Next it will be made a demo application in order to illustrate the possibilities and limitations of this rendering process in the mobile devices.

\par
Finally, the writing of this dissertation will be done throughout the entire project, but there will be a reinforced effort at the end along with the writing of an article.

\par
In short, the work plan of this dissertation should be as follows:

\begin{enumerate}

\item Survey of the State of the Art (2 weeks)
\item Requirements specification (2 weeks)
\item Development of Software Architecture (3 months)
\item Development and test of the library (2 weeks)
\item Development of a demonstration (2 weeks)
\item Writing the dissertation and an article (1 month)

\end{enumerate}

\begin{table}[H]
\centering
\caption{Scheduling for this dissertation}
\label{Scheduling for this dissertation}
\begin{tabular}{|c|c|c|c|c|c|c|}[H]
\hline
\textbf{}                               & \multicolumn{6}{c|}{\textbf{2017}} \\ \hline
                                        & Jan  & Feb & Mar & Apr & May & Jun \\ \hline
Survey of the State of the Art          & X    &     &     &     &     &     \\ \hline
Requirements specification              & X    &     &     &     &     &     \\ \hline
Development of Software Architecture    &      & X   & X   & X   &     &     \\ \hline
Development and test of the library     &      &     &     &     & X   &     \\ \hline
Development of a demonstration          &      &     &     &     & X   &     \\ \hline
Writing the dissertation and an article &      &     &     &     &     & X   \\ \hline
\end{tabular}
\end{table}

\fi



\section{Library}

\par
As stated above, this library was implemented in an object-oriented fashion.
The most important classes that provide functionalities already developed for the user to use are:

\begin{itemize}
\item Renderer: class that starts the rendering process and stores the calculated pixels colors in a C style array.
\item Scene: class that stores the geometry information in vectors and provides methods to trace rays in the scene without any acceleration structures.
\item Shapes: set of classes that allows to create triangles, spheres and planes.
\item Material: class that stores all 4 types of material color:
\begin{itemize}
	\item Emission light color
	\item Diffuse reflection color
	\item Specular reflection color
	\item Specular refraction color
\end{itemize}
\item Primitive: class that stores the shape and material of each primitive in the scene.
\item Ray: class that represents a ray casted into the scene.
\end{itemize}

\subsection{Third parties dependencies}

\par
Before describing each functionality provided in each class developed, it is important to mention that this library uses 3 other libraries developed in C++ by third parties.
Those libraries are:

\begin{itemize}
	\item OpenGL Mathematics (\cite{GLM}): used to create 3d points and vectors, perform geometry calculations and store pixels and primitives colors.
	\item tinyobjloader (\cite{tinyobjloader}): used to load scenes from wavefront obj files into the main memory.
	\item Google Test (\cite{GoogleTest}): used to create some unit tests and to mock some classes.
\end{itemize}

\par
All these libraries are Android compatible and are reliable in terms of performance and maintenance.

\par
Besides being dependent on these 3 libraries, this library also depends on OpenGL ES 2.0 (\cite{OpenGL_ES_2}) from Android SDK and depends on CMake (\cite{CMake}) in order to compile the application.

\subsection{Renderer}

\par
The Renderer is the closest class to the application that starts the rendering process.
This class provides two main methods:

\begin{lstlisting}[caption={Main methods in Renderer}, captionpos=b, label=Renderer]
void renderFrame(::std::uint32_t *bitmap, ::std::int32_t numThreads, ::std::uint32_t stride) noexcept;
void stopRender() noexcept;
\end{lstlisting}

\par
The \textit{renderFrame} method starts the rendering process and writes the calculated light luminance of each pixel in the parameter \textit{bitmap}.
This method also allows to choose the number of threads that will render the image into the bitmap, and needs the stride of that bitmap array.
The image plane is divided into 16 tiles of pixels and it is traced one primary ray per pixel in each tile.

\begin{figure}[H]
	\centering
	\caption{Illustration of tiling the image plane.}
	\label{Illustration of tiling the image plane.}
	\includegraphics[width=10cm,height=10cm,keepaspectratio,scale=1.0]{tiling.png}
\end{figure}

\par
The \textit{stopRender} method only serves to stop the rendering process without cleaning the pixels' colors already calculated.

\subsection{Scene}

\par
The Scene is the class that handles the process of intersecting a ray with the primitives and source lights in the scene.
Besides providing a vector to add the light sources and a vector to add primitives to the scene, it also provides two main methods:

\begin{lstlisting}[caption={Main methods in Scene}, captionpos=b, label=Scene]
Intersection trace(Intersection intersection, const Ray &ray) noexcept;
Intersection shadowTrace(Intersection intersection, const Ray &ray) noexcept;
\end{lstlisting}

\par
The \textit{trace}, as the name implies, is a method that tries to intersect a ray with all the primitives and light sources in the scene.
And It returns the closest intersection to the origin of that ray.
This method is used to determine the intersection of the casted ray with the primitives in the scene.

\par
The \textit{shadowTrace}, is similar to the \textit{trace} method but with the difference that returns the first intersection found.
The purpose of this method is to simulate the shadows in the scene, like it is illustrated in figure \ref{Illustration of shadow rays casting.}.

\begin{figure}[H]
	\centering
	\caption{Illustration of shadow rays casting (\cite{ShadowRays}).}
	\label{Illustration of shadow rays casting.}
	\includegraphics[width=10cm,height=10cm,keepaspectratio,scale=1.0]{Shadow_Ray.jpg}
\end{figure}


\subsection{Shapes}

\par
In order to make possible to generate scenes with all kind of objects, it was developed 3 types of shapes: plane, sphere and triangle.
All shapes provide only one method:

\begin{lstlisting}[caption={Main methods in Shape}, captionpos=b, label=Shape]
Intersection intersect(const Intersection &intersection, const Ray &ray) const noexcept;
\end{lstlisting}

This method determines if a ray intersects the shape and returns the intersection.
It is important to mention that, obviously, each shape was developed with different algorithm.

\subsubsection{Plane}

\par
The plane is an essential primitive shape because it allows the user to build indoor scenes.

\par
The construction of a plane requires just an arbitrary point in the plane and the normal of the plane.
So, the intersect method implemented has the following algorithm:

\begin{lstlisting}[caption={Algorithm of Ray plane intersection}, captionpos=b, label=Plane]
projection = planeNormal . rayDirection
if (|projection| <= 0) return false
distance = planeNormal . (planePoint - rayOrigin) / projection
if (distance <= 0 || distance > rayMaxDistance) return false
intersectionPoint = rayOrigin + rayDirection * distance
return Intersection(intersectionPoint, planeNormal, material)
\end{lstlisting}


\begin{figure}[H]
	\centering
	\caption{Illustration of a ray intersecting a plane (\cite{PlaneRayIntersection}).}
	\label{Plane.}
	\includegraphics[keepaspectratio,scale=0.4]{RayToPlane.png}
\end{figure}

\subsubsection{Sphere}

\par
The sphere is also an important primitive shape because it allows the user to build some common objects with a shape of a ball.

\par
The construction of a sphere requires just the point in the center and the radius of the sphere.
It also should be noted that a ray can intersect a sphere at two points and therefore it is necessary to determine the closest intersection point to the origin of the ray.
So, the intersect method implemented has the following algorithm:

\begin{lstlisting}[caption={Algorithm of Ray sphere intersection}, captionpos=b, label=Sphere]
centerToOrigin = rayOrigin - sphereCenter
B = 2 * centerToOrigin . rayDirection
C = centerToOrigin.magnitude - radius
disciminant = B^2 - 4*C
if (discriminant <= 0) return false
distance1 = (-B + sqrt{discriminant} * 0.5)
distance2 = (-B - sqrt{discriminant} * 0.5)
distance = min(distance1, distance2)
if (distance <= 0 || distance > rayMaxDistance) return false
intersectionPoint = rayOrigin + rayDirection * distance
sphereNormal = intersectionPoint - sphereCenter
return Intersection(intersectionPoint, sphereNormal, material)
\end{lstlisting}

\begin{figure}[H]
	\centering
	\caption{Illustration of a ray intersecting a sphere in two points (\cite{SphereRayIntersection}).}
	\label{Sphere.}
	\includegraphics[keepaspectratio,scale=0.4]{Ray_Sphere_Intersection.png}
\end{figure}

\subsubsection{Triangle}

\par
And finally, obviously the triangle has also been implemented because, as it is the simplest primitive with an area, it allows to build many different object shapes.

\par
The construction of a triangle requires three points
$[A, B, C]$
, two vectors
$[AB, AC]$
and the normal of the triangle.
So, the intersect method implemented has the following algorithm:

\begin{lstlisting}[caption={Algorithm of Ray triangle intersection}, captionpos=b, label=Triangle]
perpendicularVector = rayDirection x AC
projection = AB . perpendicularVector
if(|projection| <= 0) return false
vectorToRay = rayOrigin - A
u = (vectorToRay . perpendicularVector) / projection
if (u < 0 || u > 1) return false
perpendicularVector2 = vectorToRay x AB
v = (rayDirection . perpendicularVector2) / projection
if(v < 0 || (u + v) > 1) return false
distance = (AC . perpendicularVector2) / projection
intersectionPoint = rayOrigin + rayDirection * distance
return Intersection(intersectionPoint, triangleNormal, material)
\end{lstlisting}

\begin{figure}[H]
	\centering
	\caption{Illustration of a ray intersecting a triangle (\cite{TriangleRayIntersection}).}
	\label{Sphere.}
	\includegraphics[keepaspectratio,scale=1.0]{triangle.png}
\end{figure}

\subsection{Acceleration Structures}

\par
As previously stated, the rendering algorithms based on ray tracing can be very computationally demanding, like Path Tracing and Bidirectional Path Tracing.
These algorithms are very computationally demanding because if the scene is very complex, made with millions of primitives, it is necessary to try to intersect every ray thrown into the scene with those millions of primitives.
This task is, obviously, very time consuming and can waste a lot of battery power on the mobile device.
Luckly, there are already known techniques, called acceleration structures, that helps to accelerate this process by reducing the number of intersections.

\par
There are 2 types of approaches that acceleration structures can have:

\begin{itemize}
	\item Subdivision of Space: the 3D space of the scene is divided in smaller portions of space which the volume elements can be uniform or irregular:
	\begin{itemize}
		\item Regular Grids
		\item Octrees
		\item Kd-trees
	\end{itemize}
	\item Subdivision of Objects: the 3D space of the scene is divided by aggregating the 3D primitives in groups which are next to each others:
	\begin{itemize}
		\item Bounding Volume Hierarchy (BVH)
	\end{itemize}
\end{itemize}

\par
The idea in Subdivision of Space is allowing the intersections to start with the nearest primitives of the ray and is only intersected with the further ones, if these are not in the same direction of the ray.
These structures also allow the rapid and simultaneous rejection of groups of primitives.

\par
In contrast, structures based on Subdivision of objects only allow the rapid and simultaneous rejection of groups of primitives.

\par
The developed library provides only 2 types of structures, 1 of each approach: the regular grid and the bounding volume hierarchy.
Both structures are built with Axis Aligned Bounding Volumes (AABB), which are volume elements (voxels) that can cover, 1 or more primitives from the 3D scene.
An AABB is a voxel, like the name implies, that is aligned to the axis of the scene.
This type of voxels are great to use in acceleration structures because it is very fast to test whether or not a ray intersects that voxel and it only needs to store, in memory, 2 points of that box.
Besides AABB, it is possible to build the structures with other types of voxels, like:

\begin{itemize}
	\item Sphere: intersection with a sphere is simpler than AABB but covers more 3D space than the scene primitive needs.
	\item Oriented Bounding Volume: its like a AABB but rotated according to the orientation of the primitive.
	\item k-Discrete Oriented Prototype (k-DOP): 
	\item Convex Hull: 
\end{itemize}

\subsubsection{Regular Grid}

\par
A regular grid is an acceleration structure where the scene space is subdivided into equal voxels.
Each voxel is a volume element where it could have inside 1 or more primitives of the scene, if a ray intersects that voxel, then it tries to intersect the primitives inside it.
The order of testing each voxel is from the nearest of the ray into the furthest in the direction of that ray.
This type of structure is very fast to build but its traversal is poorly efficient due to poor distribution of primitives by voxels.
The figure \ref{RegularGrid.} shows the process of intersecting a ray with the primitives in a scene.

\begin{figure}[H]
	\centering
	\caption{Illustration of traversing a regular grid acceleration structure (\cite{RegularGrid}).}
	\label{RegularGrid.}
	\includegraphics[keepaspectratio,scale=1.0]{Regular_Grid.jpg}
\end{figure}

\par
The ray tracer library provides a regular grid as an acceleration structure and its operation is as shown in the figure.

\subsubsection{Bounding Volume Hierarchy}

\par
Bounding Volume Hierarchy is another acceleration structure provided in the library.
This structure is different from the regular grid because the space is not divided into voxels of the same size.
The space is divided in a way where the primitives of the scene are grouped according to the nearest primitives of each.
This allows to build a structure of voxels where a voxel has, for sure, 1 or more primitives inside.

\par
The figure \ref{BVH.} shows an example of a BVH structure of a scene.
As it can be seen, each primitive is covered by an AABB, and then 2 or more AABBs are covered by 1 larger AABB.
In this library, this is built in a top down manner, where the scene is divided by a fake plane that divides the scene into 2 AABBs.
The calculation of that plane is using the Surface Area Heuristic which is a very popular heuristic commonly used in ray tracers.

\begin{figure}[H]
	\centering
	\caption{Illustration of traversing a Bounding Volume Hierarchy acceleration structure (\cite{BVH}).}
	\label{BVH.}
	\includegraphics[keepaspectratio,scale=1.0]{BVH.jpg}
\end{figure}

\subsubsection{Surface Area Heuristic}

\par
As it was said above, the library uses the Surface Area Heuristic in order to split the scene.
The algorithm used in the library is very simple, it just needs to calculate the minimum area surface of the sum of left and right subtrees, as following:

\begin{equation}
SAH_{optimal} = min(S_L * N_L + S_R * N_R)
\end{equation}

\begin{conditions}
	N_L & number of polygons in left subtree \\
	N   & number of polygons in right subtree \\
	S_L & surface area of left subtree \\
	S_R & surface area of right subtree \\
\end{conditions}

\par
The figure \ref{SAH.} shows a very simple example of how the SAH algorithm works.

\begin{figure}[H]
	\centering
	\caption{Illustration of how Surface Area Heuristic works in Bounding Volume Hierarchy acceleration structure (\cite{SAH}).}
	\label{SAH.}
	\includegraphics[keepaspectratio,scale=1.0]{SAH_example.png}
\end{figure}


\section{Rendering Components}

\par
In order to show the functionalities provided by the library, it was developed a few Rendering Components.
It is provided the perspective and orthographic cameras, the area and point lights, 6 Samplers, 5 Shaders and 1 OBJ loader.

\subsection{Shaders}

\par
The implemented ray tracer was programmed objected oriented, so each Rendering Component was developed separately.
This allows the user to develop his own Rendering Components without having to develop the ray tracer engine.

\par
In this context, a shader is the Rendering Component that describes how the Rendering Equation is approximated.
The Rendering Equation describes how the total radiance reflected by any point
$p$
of a surface in a direction
$\omega$$r$
is calculated.
The Bidirectional Reflectance Distribution Function (BRDF) is a function that tries to approximate the Rendering Equation.
In summary, a shader, in this context, is the algorithm of a BRDF.

\begin{figure}[H]
	\centering
	\caption{Illustration of the rendering equation describing the total amount of light emitted from a point
	$x$
	along a particular viewing direction and given a function for incoming light and a BRDF.}
	\label{Rendering_Equation.}
	\includegraphics[width=10cm,height=10cm,keepaspectratio,scale=1.0]{Rendering_eq.png}
\end{figure}

\par
There was developed three shaders: NoShadows, Whitted and PathTracer.
All shaders provide only one main method that allows the user to calculate the RGB color of a pixel with the information of a ray and the respective intersection.

\begin{lstlisting}[caption={Main methods of Shader}, captionpos=b, label=Shader]
void shade(RGB &rgb, Intersection &intersection, Ray &ray) const override;
\end{lstlisting}

\par
The parameter
$rgb$
is where the color of the pixel will be calculated and the
$intersection$
contains the necessary information about the intersection of a ray with the primitive like the intersection point, its normal and the material of the intersected material.
Finally the parameter
$ray$
is where the information about the ray like the origin of the ray and its direction is accessed.

\subsubsection{NoShadows}

\par
NoShadows is the simplest shader because, as the name implies, it does not synthesize the shadows and it only simulates the direct lighting.
This BRDF only simulates direct lighting in primitives with diffuse surfaces and it does not take into account the light coming from other points.
As already said, the indirect lighting is not fully simulated but rather simplified in a way of fixed ambient light of about 10\% of the color of the objects.

\par
The algorithm developed is as following:

\begin{lstlisting}[caption={Algorithm of NoShadows Shader}, captionpos=b, label=NoShadows]
if (intersected material is diffuse) {
  for each light source
  	for each sample
	    vectorToLight = lightPosition - intersectionPoint
	    cos_N_L = vectorToLight . intersectionNormal
	    if (cos_N_L > 0) rgb += kD * radLight * cos_N_L
  rgb /= #samples
  rgb /= #lights
  rgb += kD * 0.1 //Ambient light
}
\end{lstlisting}

\begin{figure}[H]
	\centering
	\caption{NoShadows shader.}
	\label{NoShadows shader.}
	\includegraphics[keepaspectratio,scale=0.3]{NoShadows.png}
\end{figure}

\subsubsection{Whitted}

\par
As the name of this shader implies, Whitted is the algorithm demonstrated by Whitted in 1980s.
Like the previous shader, it doesn't simulate indirect lighting.
This BRDF calculates the reflected light on diffuse and specular surfaces, as well as the light refracted on surfaces with a degree of transmission, such as water.
In order to simulate a reflective and refractive surfaces, the algorithm was divided into each case.
This makes it possible to simulate refractive surfaces, reflective surfaces and even refractive and reflective surfaces.

\par
The reflected light in a diffuse surface is calculated by adding the casting rays in direction to the lights.
These rays are called shadow rays and their radiance are multiplied by the dot product between vector to the light and the shading normal.
At the end, the summation is multiplied by the diffuse color of the intersected primitive and divided by the number of samples taken.

\par
The reflected light in a specular surface is calculated in a different way.
The specular ray is casted and ray traced, then the obtained radiance is multiplied by the specular color of the intersected primitive.
The reflection direction is the subtraction between the double of dot product between the inverse of the ray direction and the shading normal multiplying by the shading normal, and the inverse of the ray direction.

\begin{figure}[H]
	\centering
	\caption{Reflections projected on the floor and on the spheres.}
	\label{Reflection.}
	\includegraphics[keepaspectratio,scale=0.3]{Reflection.png}
\end{figure}

\par
Finally, the refracted light in a transmission surface is calculated by using the refractive index of the intersected primitive and the ray direction and shading normal.
First, the shading normal is inverted if the origin of the ray was inside a primitive, like a sphere, and if it was not then the refractive index is inverted.
Then, it is calculated two auxiliary scalar projections:
$cosTheta1$
and
$cosTheta2$
.
$cosTheta1$
is the dot product of the inverse of the shading normal and the ray direction, and
$cosTheta2$
is the difference of 1 and refractive index squared multiplied by one minus
$cosTheta1$
squared.
Then, if
$cosTheta2$
is greater than zero, then the direction of the transmission ray is ray direction multiplied with refractive index plus shading normal multiplied by refractive index multiplied $cosTheta1$
minus square root of 
$cosTheta2$
.
Else, the direction of the transmission ray is just ray direction plus shading normal multiplied by the double of
$cosTheta1$
.
And, as usual, the transmission ray is traced and its radiance is multiplied by the transmission component of the intersected primitive.

\begin{figure}[H]
	\centering
	\caption{Refraction of light at the interface of two media of different refractive indices.}
	\label{Refraction.}
	\includegraphics[keepaspectratio,scale=0.3]{Refraction.png}
\end{figure}

\par
The algorithm developed is as following:

\begin{lstlisting}[caption={Algorithm of Whitted Shader}, captionpos=b, label=Whitted]
if (intersected material is diffuse) {
  for each light source
    for each sample
      vectorToLight = lightPosition - intersectionPoint
      cos_N_L = vectorToLight . shadingNormal
      if (cos_N_L > 0) {
        Ray shadowRay(intersectionPoint, vectorToLight, distanceToLight, rayDepth + 1)
        if (!shadowTrace(shadowRay)) rgb += radLight * cos_N_L
      }
  rgb *= kD
  rgb /= #samples
}

if (intersected material is specular reflective) {
  reflectionDir = (2 * symRayDirection . normal) * normal - symRayDirection
  Ray specularRay(intersectionPoint, reflectionDir, rayDepth + 1)
  rgb += rayTrace (specularRay) * kS
}

if (intersected material is specular refractive) {
  if (shadingNormal . rayDirection > 0) {
    //we are inside the medium
    shadingNormalT = -1 * shadingNormal
    n = 1 / n
  }
  n = 1 / n
  cosTheta1 = -shadingNormalT . rayDirection
  cosTheta2 = 1 - n^2*(1-cosTheta1^2)
  if (cosTheta2 > 0)
    transmissionRay.dir = ((rayDir*n) + (shadingNormalT*(n*cost1 - sqrt(cost2))))
  else
    transmissionRay.dir = (rayDir + shadingNormalT*(cost1 * 2))
  rgb += rayTrace(transmissionRay) * kT
}
\end{lstlisting}

\begin{figure}[H]
	\centering
	\caption{Whitted shader.}
	\label{Whitted shader.}
	\includegraphics[keepaspectratio,scale=0.3]{Whitted.png}
\end{figure}


\subsubsection{PathTracer}

\par
Finally, the last shader developed is the canon Path Tracer.
Unlike the previous shaders, this one fully simulates both direct and indirect lighting.
But, like the previous shader, this algorithm simulates the light reflected on diffuse and specular surfaces, as well as the light refracted on transparent primitives.

\par
The light reflected on diffuse surfaces is divided in two parts: direct lighting and indirect lighting.
The direct lighting is calculated in a similar way to the Whitted shader but with the difference that samples are not taken from all sources of light but rather only one.
The light source in each sample is chosen randomly.
Then the obtained light radiance is multiplied by the Probability Density Function (PDF) in order to approximate the expected value.
In this case the PDF is as simple as
$1 / \#lights$.

\par
Finally, the indirect lighting reflected in diffuse surfaces is calculated in a much more complex way.
First it is generated, from the intersected point, a random direction on an unit hemisphere with a PDF proportional to cosine-weighted solid angle\\
($PDF: p(\Theta) = cos\theta / \pi$,
$x = cos(2\pi r1)\sqrt{1-r2}$,
$y = sin(2\pi r1)\sqrt{1-r2}$,
$z = \sqrt{r2}$)
.
This kind of PDF is one of the best ways to render images with path tracing that the rendering equation converges to the expected value as fast as possible.


\iffalse
Then a rotation matrix is built, where it lets rotate a vector around an arbitrary axis with a given angle.

\begin{figure}[H]
	\centering
	\caption{Rotation matrix from an arbitrary axis and an angle.}
	\label{Rotation_matrix.}
	\includegraphics[keepaspectratio,scale=0.5]{Rotation_Matrix.png}
\end{figure}

Then the proper coordinates in the world view is just: $globalCoordinates = localCoordinates * rotationMatrix$.
\fi

Then to generate 


And the indirect lighting on diffuse surfaces is then the multiplication of the ray traced light radiance with the color of the primitive and
$\pi$
, and divided by the probability of stopping the Russian roulette.

\par
The reflected light in a specular surface and the refracted light are simulated in a similar way to the Whitted algorithm presented previously.

\par
The algorithm developed is as following:

\begin{lstlisting}[caption={Algorithm of Path Tracer Shader}, captionpos=b, label=PathTracer]
if (intersected material is diffuse) {
  for each light
    for each sample
      light = samplerLight.getSample()
      vectorToLight = lightPosition - intersectionPoint
      cos_N_L = vectorToLight . shadingNormal
      if (cos_N_L > 0) {
        Ray shadowRay(intersectionPoint, vectorToLight, distanceToLight, rayDepth + 1)
        if (!shadowTrace(shadowRay)) rgb += radLight * cos_N_L
      }
  rgb *= kD
  rgb *= #lights
  rgb /= #samples

  if (rayDepth <= RAY_DEPTH_MIN || uniform_dist(gen) > finish_probability) {
    local = Generate random direction on unit hemisphere 
    proportional to cosine-weighted solid angle

    RotationMatrix = Rotation matrix from axis and angle

    Vector3D up = (0,1,0)
    Vector3D u = intersectionNormal x up
    if (u == 0) u.x = 1
    cosTheta = up . intersectionNormal
    sinTheta = 1 - cosTheta^2
    if (sinTheta < 0) sinTheta *= -1
    sinTheta = sqrt(sinTheta)

    global = local * rotationMatrix

    Ray secundaryRay (globalX, globalY, globalZ, intersectionPoint, rayDepth + 1)

    LiD = rayTrace(secundaryRay) * kD * PI

    if (rayDepth > RAY_DEPTH_MIN) LiD /= continue_probability
    if(secundaryRay intersects light) LiD = 0

    rgb += LiD
  }

}

if (intersected material is specular reflective) {
  reflectionDir = (2 * symRayDirection . normal) * normal - symRayDirection
  Ray specularRay(intersectionPoint, reflectionDir, rayDepth + 1)
  rgb += rayTrace (specularRay) * kS
}

if (intersected material is specular refractive) {
  if (shadingNormal . rayDirection > 0) {
    //we are inside the medium
    shadingNormalT = -1 * shadingNormal
    n = 1 / n
  }
  n = 1 / n

  cosTheta1 = -shadingNormalT . rayDirection
  cosTheta2 = 1 - n^2*(1-cosTheta1^2)

  if (cosTheta2 > 0)
    transmissionRay.dir = ((rayDir*n) + (shadingNormalT*(n*cosTheta1 - sqrt(cosTheta2))))
  else
    transmissionRay.dir = (rayDir + shadingNormalT*(cosTheta1 * 2))

  rgb += rayTrace(transmissionRay) * kT
}
\end{lstlisting}

\begin{figure}[H]
	\centering
	\caption{PathTracer shader.}
	\label{PathTracer shader.}
	\includegraphics[keepaspectratio,scale=0.3]{PathTracer.png}
\end{figure}


\subsection{Samplers}

\par
There were implemented four samplers: Constant, Stratified, HaltonSequence and Random.

\par
All samplers just provide one method for the user to use:

\begin{lstlisting}[caption={Main methods of Sampler}, captionpos=b, label=Sampler]
float getSample(const unsigned int sample);
\end{lstlisting}

\par
The
$getSample$
method receives as parameter the number of the current sampler and returns the the actual sample.
An atomic variable
$sample$
is used to count the current index of the samples.

\subsubsection{Constant}

\par
This sampler is the simplest because it always returns the same number passed to the constructor.

\par
The algorithm of the
$getSample$
is just:

\begin{lstlisting}[caption={Algorithm of Constant Sampler}, captionpos=b, label=Constant]
return value
\end{lstlisting}


\subsubsection{Stratified}

\par
This sampler makes each sample at equal distance
$(1 / domainSize)$
and in ascending order.
For example, for a domain size of 4, the samples taken are going to be: 0, 0.25, 0.5 and 0.75.

\par
The algorithm of the
$getSample$
is then:

\begin{lstlisting}[caption={Algorithm of Stratified Sampler}, captionpos=b, label=Stratified]
//atomic operation
current = sample++
if (current >= (domainSize * (sampleParam + 1))) {
  sample--
  return 1
}
task = current - (sampleParam * domainSize)
return task / domainSize
\end{lstlisting}


\subsubsection{HaltonSequence}

\par
As the name implies, this sampler generates the Halton sequence.

\par
Halton sequence is a quasi random number sequence which is a deterministic sequence with low discrepancy.

These sequences are usually good for Rendering Algorithms like Path Tracing because it can make the rendering equation converge faster (with fewer samples).

\begin{figure}[H]
	\centering
	\caption{Halton sequence in a 2D image plane.}
	\label{Halton_sequence_2D.}
	\includegraphics[keepaspectratio,scale=0.5]{Halton_sequence_2D.png}
\end{figure}

\par
The algorithm of the
$getSample$
is as following:

\begin{lstlisting}[caption={Algorithm of HaltonSequence Sampler}, captionpos=b, label=HaltonSequence]
//atomic operation
current = sample++
if (current >= (domainSize * (sampleParam + 1))) {
  sample--
  return 1
}
task = current - (sampleParam * domainSize)

f = 1
result = 0
while (task > 0) {
  f = f / base
  result = result + f * (index % base)
  task = floor(index / base)
}
return result
\end{lstlisting}

\subsubsection{Random}

\par
This sampler is just a wrapper to call the function rand() of the standard C library.

This function is a pseudo random number generator (PRNG), which is an algorithm for generating a sequence of numbers whose properties approximate the properties of sequences of random numbers.
Although the PRNG-generated sequence is not truly random, because it is completely determined by an initial value, called the PRNG's seed.
These sequences are usually used in simulations that use Monte Carlo methods like the Monte Carlo Ray Tracing.

\begin{figure}[H]
	\centering
	\caption{Pseudorandom sequence in a 2D image plane.}
	\label{Pseudorandom_sequence_2D.}
	\includegraphics[keepaspectratio,scale=0.5]{Pseudorandom_sequence_2D.png}
\end{figure}

\par
So the algorithm of the
$getSample$
is just:

\begin{lstlisting}[caption={Algorithm of Random Sampler}, captionpos=b, label=Random]
return rand() / RAND_MAX
\end{lstlisting}


\subsection{Lights}

\par
There were implemented two types of light sources: PointLight and AreaLight.
These light sources provide the user two main methods:

\begin{lstlisting}[caption={Main methods in Light}, captionpos=b, label=Light]
const Point3D getPosition(void);
bool intersect(Intersection &intersection, const Ray &ray, const Material &) const;
\end{lstlisting}

\par
The
$getPosition$
method just returns the position of the light source and the 
$intersect$
method determines whether a given
$ray$
as a parameter intersects this light source and, if it intersects, writes the result to the
$intersection$
parameter.

\subsubsection{PointLight}

\par
The PointLight is the simplest form of a light because, as the name implies, is just a point of light.

\par
Therefore, the
$getPosition$
method is very simple because it only returns the position of the light source determined in its constructor:

\begin{lstlisting}[caption={Algorithm of PointLight}, captionpos=b, label=PointLight]
return position;
\end{lstlisting}

\par
And the
$intersect$
method is also quite simple because it always returns false.
This is because the probability of a ray intersecting a single 3D point in the world is practically nil.

\subsubsection{AreaLight}

\par
The AreaLight implemented has a shape of a triangle.
This shape is intended as it allows to generate a random point in a triangle with barycentric coordinates.

\par
A triangle with 3 points: A, B and C.
We get vectors AB and AC, being:

\begin{lstlisting}[caption={Algorithm of AreaLight}, captionpos=b, label=AreaLight]
AB=[Bx-Ax,By-Ay,Bz-Az] and AC=[Cx-Ax,Cy-Ay,Cz-Az].
\end{lstlisting}

\par
These vectors tell how to get from point A to the other two points in the triangle, by telling us what direction to go and how far.
So, with barycentric coordinates R=1/3, S=1/3 and T=1/3, we get the point in the center of the triangle.
To generate a random point in the triangle, we have to generate 2 random numbers between 0 and 1 (R and S).
Then we have to make sure we stay inside the triangle by checking if they are larger than one:

\begin{lstlisting}[caption={Algorithm of AreaLight}, captionpos=b, label=AreaLight]
if (R + S >= 1) {
  R = 1 - R
  S = 1 - S
}
\end{lstlisting}

Finally we can obtain a random point in the triangle by starting at point A, then getting a random percentage along vector AB and a random percentage along vector AC:

\begin{lstlisting}[caption={Algorithm of AreaLight}, captionpos=b, label=AreaLight]
RandomPointPosition = A + R*AB + S*AC
\end{lstlisting}

\begin{figure}[H]
	\centering
	\caption{Calculation of point P by using barycentric coordinates starting at point A and adding a vector AB and a vector AC.}
	\label{Barycentric_Coordinates.}
	\includegraphics[keepaspectratio,scale=0.5]{barycentric-coordinates.png}
\end{figure}

\par
So, the
$getPosition$
algorithm is as following:

\begin{lstlisting}[caption={Algorithm of AreaLight}, captionpos=b, label=AreaLight]
R = samplerPointLight.getSample(0)
S = samplerPointLight.getSample(0)
if (R + S >= 1) {
  R = 1 - R
  S = 1 - S
}
x = A.x + R * AB.x + S * AC.x
y = A.y + R * AB.y + S * AC.y
z = A.z + R * AB.z + S * AC.z
return Point3D(x, y, z)
\end{lstlisting}

\par
And the
$intersect$
method is equal to the intersection of a ray with a triangle presented earlier.

\subsection{Cameras}

\par
Only 2 types of cameras were implemented: perspective and orthographic cameras.

\par
The camera only provides one method for the user to cast a ray from the camera position in direction to the image plane:

\begin{lstlisting}[caption={Main methods in Camera}, captionpos=b, label=Camera]
Ray generateRay(const float u, const float v,
                const float deviationU, const float deviationV) const;
\end{lstlisting}

\par
The method
$generateRay$
needs four parameters to create a ray.
The $u$ and $v$
are used to choose the targeted pixel in the image plane.
Being
$u$
the inverse of the index of the pixel in its line, that is,
$x / width$
, and
$v$
the inverse of the index of the pixel in its column, that is,
$y / height$.
In order to allow the reduction of aliasing in the generated images of the scene, the camera also accepts two extra parameters
$deviationU$ and $deviationV$
which are variances inside a pixel.
The
$deviationU$
is a horizontal variance of the pixel, that is,\\
$[-0.5*pixelWidth, 0.5*pixelWidth]$
and
$deviationV$ the variance in the vertical of the pixel\\
$[-0.5*pixelHeight, 0.5*pixelHeight]$.
This technique is called jittering, as shown in figure \ref{Jittering.}, and allows to reduce the aliasing effect by introducing noise into the output image.
Although the final image gets some noise, this effect turns out to be visually more appealing than aliasing because it is an effect without patterns that are easily detectable by the human eye.
It is important to reduce the aliasing effect because, it is an effect that the human eyes can detect very fast because the generated image will have quite regular patterns, as shown in figure \ref{Aliased.}.

\begin{figure}[H]
	\centering
	\caption{Illustration of how sampling the plane image with jittering works.}
	\label{Jittering.}
	\includegraphics[keepaspectratio,scale=1.0]{Jittering.png}
\end{figure}

\begin{figure}[H]
	\centering
	\caption{Aliasing effect.}
	\label{Aliased.}
	\includegraphics[keepaspectratio,scale=0.6]{aliasing.jpg}
\end{figure}


\par
Sampling with jittering is, as previously stated, a good technique to avoid aliasing in the image.
The figure \ref{Sampling_Stratified.} shows an example of stratified sampling with 1 and 256 samples per pixel.
As can be seen in the figure on the left, there is a noticeably aliasing in the image, and we can't even perceive the correct positions of the black squares on the background.
Of course, with 256 samples per pixel, the aliasing effect is greatly reduced, but also, the execution time is linearly increased, which is a downside to the user experience.

\begin{figure}[H]
	\centering
	\subfloat[Stratified sampling with 1, sampler per pixel.][Stratified sampling with 1 \\sample per pixel.]{{\includegraphics[width=5cm]{regular_1spp.png} }}
	\qquad
	\subfloat[Stratified sampling with 256, samplers per pixel.][Stratified sampling with 256 \\samples per pixel.]{{\includegraphics[width=5cm]{regular_256spp.png} }}
	\caption{Stratified sampling.}
	\label{Sampling_Stratified.}
\end{figure}

\par
Sampling with jittering is, obviously not perfect, as shown in the figure \ref{Sampling_Jittering.}.
But, it produces more visually appealing images, although it introduces some noise.
Even, with just 1 sample per pixel, the generated image is less "jaggy" in the background.
And, with 4 samples per pixel, the noise is greatly reduced and its quality is visually comparable to the stratified sampling with 256 samples per pixel.
This means that by using jittering it is possible to obtain images pleasing to the human eye with fewer samples per pixel, and consequently in less execution time.

\begin{figure}[H]
	\centering
	\subfloat[Jittered sampling with 1, sampler per pixel.][Jittered sampling with 1 \\sample per pixel.]{{\includegraphics[width=5cm]{jittering_1spp.png} }}
	\qquad
	\subfloat[Jittered sampling with 4, samplers per pixel.][Jittered sampling with 4 \\samples per pixel.]{{\includegraphics[width=5cm]{jittering_4spp.png} }}
	\caption{Jittered sampling.}
	\label{Sampling_Jittering.}
\end{figure}


\subsubsection{Perspective Camera}

\par
With this type of camera we can simulate images being seen by the human eyes, which means, it simulates the depth of the objects, and produces 2D images with a 3D projection.

\par
To obtain an image plane with perspective is necessary to have a Field of View.
In order to accept any resolution of the image plane, we have to divide the field of view in 2 parts: horizontal and vertical.
This way, we can obtain the aspect ratio of the image plane we want.

\begin{figure}[H]
	\centering
	\caption{Perspective camera with hFov and vFov (\cite{Camera_Perspective_Orthographic}).}
	\label{Perspective_Camera.}
	\includegraphics[keepaspectratio,scale=0.6]{Camera_Perspective_v2.png}
\end{figure}

\par
The algorithm to generate a ray from the perspective camera is very simple.
By knowing the horizontal Field of View and vertical Field of View in radians, and with
$u$
and
$v$
as parameters, it is possible to calculate the direction of that ray.

\par
It starts to calculate the distance to go through the right vector and the up vector of the camera.
That can be done with the arctangent of each Field of View of the camera (horizontal and vertical) and multiplying it with
$u - 0.5$
in the right vector and with
$0.5 - v$
in the up vector.
This makes sure that we go through every pixel, starting with the pixel from the upper left corner to the lower right corner of the image plane.
Then the destination point of the ray is just the sum:\\
$destinationPoint = cameraPosition + cameraDirection + rightVector + upVector$
, and so its direction is just:\\
$rayDirection = destinationPoint - cameraPosition$
.
The origin of the ray is the position of the camera, because in a perspective camera, all rays come from the same point: the point where the camera is located.

\begin{lstlisting}[caption={Algorithm of Perspective Camera}, captionpos=b, label=Perspective]
rightFactor = arctan(hFov * (u - 0.5)) + deviationU
rightVector = right * rightFactor
upFactor = arctan(vFov * (0.5 - v)) + deviationV
upVector = up * upFactor
destinationPoint = cameraPosition + cameraDirection + rightVector + upVector
rayDirection = destinationPoint - cameraPosition
return Ray (rayDirection, cameraPosition)
\end{lstlisting}


\subsubsection{Orthographic Camera}

\par
The orthographic camera removes the sense of perspective by drawing the image plane without simulating the depth of the objects, making all the objects looking flat.
This is achieved by inverting the logic of the perspective camera.
Instead of calculating the direction and always having the same origin, in the orthographic camera, the direction is always the same for all rays, and the origin of the ray varies.

\par
It starts to calculate the distance to go through the right vector and up vector of the camera, like the perspective camera.
But, instead of using the Field of View, we now use the
$sizeH$
and
$sizeV$
, which are the horizontal and vertical sizes of the image plane.
Then, to make sure that we go through all pixels from upper left pixel to lower right, we need to use the
$u$
and
$v$
values from the parameters.
Then, the origin of the ray is:
$rayOrigin = cameraPosition + rightVector + upVector$
.

\begin{lstlisting}[caption={Algorithm of Orthographic Camera}, captionpos=b, label=Orthographic]
rightFactor = (u - 0.5) * sizeH
rightVector = right * rightFactor + right * deviationU
upFactor = (0.5f - v) * sizeV
upVector = up * upFactor + up * deviationV
rayOrigin = cameraPosition + rightVector + upVector
return Ray (cameraDirection, rayOrigin)
\end{lstlisting}

\begin{figure}[H]
	\centering
	\caption{Orthographic camera with sizeH and sizeV (\cite{Camera_Perspective_Orthographic}).}
	\label{Perspective_Camera.}
	\includegraphics[keepaspectratio,scale=0.6]{Camera_Orthographic_v2.png}
\end{figure}


\subsection{Object Loaders}

\par
Last, but not least, the library allows to develop different Object Loaders.
An Object Loader is, as the name implies, a class that allows the ray tracer to import meshes from different Model file formats.
There are many different types of file formats for storing meshes:

\begin{itemize}
	\item 3ds
	\item FBX
	\item Wavefront .obj file
	\item COLLADA
	\item SketchUp
	\item AutoCAD DXF
\end{itemize}

\par
All these file formats allows the user to store the positions of many triangles in a file, which together form the geometry of the scene.

\par
This library only provides an Object Loader that allows loading the scene geometry from Wavefront obj files.
This type of Model file format is a good choice because, besides being simple, it is open and has been adopted by many 3D graphics application vendors, like, 3ds Max and Blender.

\subsubsection{OBJ Loader}

\par
The OBJ file format is a simple data format that allows to store only the position of each vertex, the UV position of each texture coordinate vertex, vertex normals, and the faces that make each polygon defined as a list of vertices, and texture vertices.
Vertices are stored in a counter-clockwise order by default, making explicit declaration of face normals unnecessary.

\begin{lstlisting}[caption={.OBJ file format}, captionpos=b, label=OBJ]
	# This is a comment

	# List of geometric vertices
	# x   y   z   w
	v 1.0 2.0 3.0 1.0
	v 5.0 6.0 7.0 1.0
	...
	
	# List of texture coordinates
	#  x   y   w
	vt 0.5 1.0 0.0
	vt 0.1 0.3 0.0
	...
	
	# List of vertex normals
	#  x   y   z
	vn 0.3 0.0 0.7
	vn 0.2 1.0 0.3
	...
	
	# List of polygonal face elements
	# v1/vt1/vn1	v2/vt2/vn2	v3/vt3/vn3
	f 3/1/1		4/2/3		5/3/4
	f 6/6/3		7/1/2		8/2/6
	...
\end{lstlisting}

\par
The Wavefront OBJ file format is described as shown in \ref{OBJ}, and is accompanied by another file, Material Template Library (MTL), that describes the visual aspects of the polygons.
It is this file that describes surface shading (material) properties of objects within one or more .OBJ files.
It allows to define multiple materials, with, ambient color, diffuse color, specular reflective color and specular refractive color.
And even allows to load texture maps stored in TGA files.
The Listing \ref{MTL} shows an example of a description of a material.

\begin{lstlisting}[caption={.MTL file format}, captionpos=b, label=MTL]
# This is a comment

# Define a material named 'Colored'
newmtl Colored

# Ambient color
Ka 1.0 0.0 0.0	# red

# Diffuse color
Kd 0.0 1.0 0.0	# green

# Specular color
Ks 0.0 0.0 1.0	# blue

# Dissolved (transparency)
d 0.4
\end{lstlisting}

\section{Android specifics and challenges}

\subsection{Specifics}

\par
Before developing the ray tracer to Android, it is necessary to understand how an Android application works.

\par
A typical Android application is programmed in Java programming language.
And it usually needs to communicate with an User Interface which also is programmed in Java.
The code is compiled with Android Software Development Kit (SDK) along with any data and resource files into an Android package (APK).
But, in order to avoid using the Java Emulator in our code and program it in native code, it is necessary to use Android Native Development Kit (NDK).
Unfortunately, the NDK tool-chain doesn't provide any User Interface libraries like Qt that facilitates the build of a User Interface in native code.
So, in order to obtain the best performance possible in a mobile device, the ray tracer library and components were programmed in C++ and the User Interface was programmed in Java.

\subsection{User Interface}

\par
The Android User Interface is programmed in Java and only one thread can refresh the User Interface (UI thread).

\par
There are only 2 goals in the User Interface: should be as simple as possible and should be able to allow progressive ray tracing, which means, refreshing the image while it is being rendered.

\par
In order to allow progressive ray tracing, it was used a pool of 1 thread called Render Task thread that every 250 ms wakes up the thread and updates the strings used in the text view.
Before it finishes, it publishes the progress on the UI thread.
Then the UI thread wakes up and updates the text view.

\begin{figure}[H]
	\centering
	\caption{Execution flow of UI thread and Render Task thread.}
	\label{Execution flow of UI thread and Render Task thread}
	\includegraphics[keepaspectratio,scale=0.5]{UI_thread.png}
\end{figure}

\subsection{Challenges}

\par
Developing applications for mobile devices have different challenges compared with the traditional personal computer hardware.

\par
The Android User Interface has some particularities like only 1 thread can modify the UI, and is usually called the UI thread.

\par
The size of RAM available for executing programs is typically smaller and the CPU microarchitecture is generally simpler and with smaller computational power.
Also the Operating System (OS) is shaped for the mobile world, making a lot of restrictions in the performance of the applications in order to save battery.
The amount of main memory available for the applications can also be affected by the OS.

\par
Other challenges are related with the communication mechanism between the SDK and the NDK, because two different languages need to "communicate" in runtime (GUI in Java and the library in C++).
This involves learning how to use the Java Native Interface (JNI), so that the native code can send and receive information from Java Virtual Machine (JVM).

\par
Because ray tracing may involve rendering complex scenes, placing all this information in the main memory may not be possible, and so this memory management can be a worthy challenge.