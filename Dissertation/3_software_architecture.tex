\chapter{Software architecture}

\section{Approach}

\par
The development of this demonstration application involves the distinction of three layers of abstraction: user interface, rendering components and the library itself.

\par
The top layer is the User Interface which is obviously application, and eventually device, dependent.
The user interface of this dissertation's demo application is very simple and just allows the user to see the rendered image and choose some rendering components to use, like the integrator, the sampler, the number of threads and samples and choose the scene to render.
Although this layer is useful, this dissertation focuses only on the middle and bottom layers.

\begin{figure}[H]
	\centering
	\caption{Illustration of the developed User Interface.}
	\label{UI.}
	\includegraphics[width=10cm,height=10cm,keepaspectratio,scale=1.0]{UI.png}
\end{figure}

\par
The middle layer provides the rendering components, which are abstract concepts about rendering, that use functionalities that the library itself offers to the programmers.
Some of these rendering components are the camera, the light, the sampler, the integrator and the object loader.
These rendering components are useful for the programmer since it allows them to use features without having the need to know how these were developed.
And, of course, this facilitates and accelerates the development of new rendering applications for the programmers.

\par
Lastly, the bottom layer is the library itself, which contains the business logic of basic features in a renderer, that the rendering components use.
These features are the basic functionalities of a ray tracing engine.
Those functionalities can be: create vectors and points; create different shapes like triangles, spheres and planes; create different primitives with different materials and with different shapes; cast rays; and even intersect rays with the primitives.

\par
It is also important to mention that the demo application was built in order to show the developed features, the performance achieved in the mobile devices and also to help promote the library.
This demo application is a good way to test it in several Android devices, like: a smart phone, a tablet computer, a smart TV, or even a smart watch.

\begin{figure}[H]
	\centering
	\caption{Illustration of the three layers in the application.}
	\label{Illustration of the three layers in the application.}
	\includegraphics[width=10cm,height=10cm,keepaspectratio,scale=1.0]{Layers.png}
\end{figure}

\par
Besides the abstraction layers, there are some important strategic decisions made in order to guide the progress of the development of this library.

\par
The first decision was: the primary rays always have origin in the camera.
This decision was made in order to not mix the code of the integrator with the ray tracer renderer engine.

\par
Other decision made was to make the rendering process progressive, which means that the rendered image is incrementally refined with more and more traced rays.
As the integrator will be converging to better values.
This is important in order to give the user a fast rendered image, with some noise or aliasing, and converge it progressively to a better solution with higher details and practically without any visual noise or aliasing.

\par
Last, but not least, is that the code was developed in a modular way, in this case was programmed in an object oriented manner.
This allows the programmers to code their own rendering components, like the integrator, camera, sampler and light, without having to develop the basic features in the renderer engine.

\section{Methodology}

\par
In order to take advantage of most of the mobile CPU resources and give a good performance for the applications, the library and the rendering components were developed using the native programming language C++.
This was achieved by using the Native Development Kit (NDK) provided by the Integrated Development Environment (IDE) Android Studio.
This way, the mobile device can provide better performance in computationally intensive applications because it doesn't need the Java garbage collector.
And it facilitates the porting of existing C/C++ code to Android and promotes developing multi-platforms applications.
But, it won't ensure device portability despite processor architecture and the number of libraries compatible with the NDK is very scarce.
It is also important to mention that the User Interface was developed in Java using the traditional Software Development Kit (SDK) provided by the Android Studio because there is no framework in the NDK that helps the programmer design his own User Interface.
Despite that, its performance is not very important because it doesn't interfere significantly with the others layers of the application.



\iffalse

\section{Work plan}

\par
The development of this dissertation will be done in six stages.

\par
The first stage involves the survey of state of the art about ray tracers built for mobile systems, in order to understand the capacities and limitations of these devices when running rendering algorithms.

\par
The second stage is to make the requirements specification in order to:

\begin{itemize}
\item Describe the functionalities of the library.
\item Explain how the user interact with the external interfaces.
\item Demonstrate the design constraints imposed on an implementation of a custom rendering component.
\end{itemize}

\par
The third stage will address the development of the architecture of the entire system.
The study performed in the previous stages will serve as the basis for this development, where this architecture should be able to address all the possible problems that might arise in the development process.

\par
The fourth stage will be the development and test of the software, where it should be relatively quick if the previous stage was done properly.

\par
Next it will be made a demo application in order to illustrate the possibilities and limitations of this rendering process in the mobile devices.

\par
Finally, the writing of this dissertation will be done throughout the entire project, but there will be a reinforced effort at the end along with the writing of an article.

\par
In short, the work plan of this dissertation should be as follows:

\begin{enumerate}

\item Survey of the State of the Art (2 weeks)
\item Requirements specification (2 weeks)
\item Development of Software Architecture (3 months)
\item Development and test of the library (2 weeks)
\item Development of a demonstration (2 weeks)
\item Writing the dissertation and an article (1 month)

\end{enumerate}

\begin{table}[H]
\centering
\caption{Scheduling for this dissertation}
\label{Scheduling for this dissertation}
\begin{tabular}{|c|c|c|c|c|c|c|}[H]
\hline
\textbf{}                               & \multicolumn{6}{c|}{\textbf{2017}} \\ \hline
                                        & Jan  & Feb & Mar & Apr & May & Jun \\ \hline
Survey of the State of the Art          & X    &     &     &     &     &     \\ \hline
Requirements specification              & X    &     &     &     &     &     \\ \hline
Development of Software Architecture    &      & X   & X   & X   &     &     \\ \hline
Development and test of the library     &      &     &     &     & X   &     \\ \hline
Development of a demonstration          &      &     &     &     & X   &     \\ \hline
Writing the dissertation and an article &      &     &     &     &     & X   \\ \hline
\end{tabular}
\end{table}

\fi



\section{Library}

\par
As stated above, this library was implemented in an object-oriented fashion.
The most important classes that provide functionalities already developed for the user to use are:

\begin{itemize}
\item Renderer: class that starts the rendering process and stores the calculated pixels colors in a C style array.
\item Scene: class that stores the geometry information in vectors and provides methods to trace rays in the scene without any acceleration structures.
\item Shapes: set of classes that allows to create triangles, spheres and planes.
\item Material: class that stores all four types of material color:
\begin{itemize}
	\item Emission light color
	\item Diffuse reflection color
	\item Specular reflection color
	\item Specular refraction color
\end{itemize}
\item Primitive: class that stores the shape and material of each primitive in the scene.
\item Ray: class that represents a ray casted into the scene.
\end{itemize}

\subsection{Third parties dependencies}

\par
Before describing each functionality provided in each class developed, it is important to mention that this library uses four other libraries developed in C++ by third parties.
Those libraries are:

\begin{itemize}
	\item OpenGL Mathematics (\cite{GLM}): used to create 3d points and vectors, perform geometry calculations and store pixels and primitives' colors.
	Before using this library, it was implemented our own mathematics library and its performance was very bad compared with this library.
	\item tinyobjloader (\cite{tinyobjloader}): used to load scenes from wavefront obj files into the main memory.
	\item Google Test (\cite{GoogleTest}): used to create some unit tests and to mock some classes.
	This is important in order to have a safe and maintainable code.
	\item C++ Standard Library (\cite{C++_stdlib}): used in order to save the data in popular containers like the array and the vector.
	It also provides some useful utilities like the unique pointers in the Dynamic memory management utilities, the String and the Atomic operations used in the Samplers.
\end{itemize}

\par
All these libraries are Android compatible and are reliable in terms of performance and maintenance.
Besides being dependent on these four libraries, this library also depends on OpenGL ES 2.0 (\cite{OpenGL_ES_2}) from Android SDK and depends on CMake (\cite{CMake}) in order to compile the application.

\par
The OpenGL ES 2.0 is necessary in order to let the application rasterize one frame of the scene so the user can view it without having to wait a long time to render it with the ray tracer.
For example, in the Conference scene, the OpenGL in the Samsung Galaxy Fresh Duos can rasterize one frame in less than 17ms compared with 500ms that the NoShadows shader takes.
This is done by copying the scene data from native code to the OpenGL of JVM with the help of JNI.

\begin{figure}[H]
	\centering
	\caption{The Conference scene rendered with OpenGL ES 2.0 before ray tracing the scene.}
	\label{The Conference scene rendered with OpenGL ES 2.0 before ray tracing the scene.}
	\includegraphics[width=10cm,height=10cm,keepaspectratio,scale=1.0]{OpenGL_Conference.png}
\end{figure}

\par
The CMake was used so that the application code could be compiled on any operating system.
As it is a cross-platform family of tools designed to build, test and package software developed for a variety of operating systems like Windows, Linux, Mac OS X, and even FreeBSD.
The main goal of CMake is to control the software compilation process using simple platform and compiler independent configuration files, and generate native makefiles and workspaces that can be used in the compiler environment of the programmer's choice.
This facilitates the integration of third party libraries that use CMake to compile into our own project.
But, unfortunately the NDK only provides CMake up to the version 3.6.0, so projects that use latter versions of CMake will not work.

\par
The CMake generate makefiles that call the available C++ compilers in the Android Environment.
The NDK in Android Studio uses, by default, the LLVM Project, which is a collection of modular and reusable compiler and toolchain technologies.
This toolchain provides Clang, which is C/C++/Objective-C compiler that aims to deliver amazingly fast compiles and extremely useful error and warning messages for the programmers.
Unfortunately, this compiler only provides features up to C++ 17, so developers can't use the most recent features of C++ 20.

\par
Last, but not least, it is important to mention that all the code of this application, including the ray tracer library, the rendering components and the Graphical User Interface are available in the github, which is a web-based hosting service for version control using Git.
It is 100\% open source and it is accessed in the web page:
\url{https://github.com/PTPuscas/MobileRayTracer}.

\subsection{Renderer}

\par
The Renderer is the closest class to the application that starts the rendering process.
This class provides two main methods:

\begin{lstlisting}[caption={Main methods in Renderer}, captionpos=b, label=Renderer]
void renderFrame(std::uint32_t *bitmap, std::int32_t numThreads, std::uint32_t stride) noexcept;
void stopRender() noexcept;
\end{lstlisting}

\par
The \textit{renderFrame} method starts the rendering process and writes the calculated light luminance of each pixel in the array parameter \textit{bitmap}.
This method also allows to choose the number of threads that will render the image into the bitmap, and needs the \textit{stride} of that bitmap array.
Besides rendering the scene in a progressive way, the image plane is divided into 256 tiles of pixels and it is traced one primary ray per pixel in each tile.
This allows the user to wait less time in order to view the scene being rendering.

\begin{figure}[H]
	\centering
	\caption{Illustration of tiling the image plane.}
	\label{Illustration of tiling the image plane.}
	\includegraphics[width=10cm,height=10cm,keepaspectratio,scale=1.0]{tiling.png}
\end{figure}

\par
The \textit{stopRender} method only serves to stop the rendering process without resetting the pixels' colors already calculated.

\subsection{Scene}

\par
The Scene is the class that handles the process of intersecting a ray with the primitives and source lights in the scene.
Besides providing a vector to add the light sources and a vector to add primitives to the scene, it also provides two main methods:

\begin{lstlisting}[caption={Main methods in Scene}, captionpos=b, label=Scene]
Intersection trace(Intersection intersection, const Ray &ray) noexcept;
Intersection shadowTrace(Intersection intersection, const Ray &ray) noexcept;
\end{lstlisting}

\par
The \textit{trace}, as the name implies, is a method that tries to intersect a ray with all the primitives and light sources in the scene.
And it returns the closest intersection to the origin of that ray.
This method is used to determine the intersection of the casted ray with the primitives in the scene.

\par
The \textit{shadowTrace}, is similar to the \textit{trace} method but with the difference that returns the first intersection found.
The purpose of this method is to simulate the shadows in the scene, like it is illustrated in figure \ref{Illustration of shadow rays casting.}.
The shadow ray is represented by the yellow line pointed to the light source.

\begin{figure}[H]
	\centering
	\caption{Illustration of shadow rays casting (\cite{ShadowRays}).}
	\label{Illustration of shadow rays casting.}
	\includegraphics[width=10cm,height=10cm,keepaspectratio,scale=1.0]{Shadow_Ray.jpg}
\end{figure}


\subsection{Shapes}

\par
In order to make possible to generate scenes with all kind of objects, it was developed three types of shapes: plane, sphere and triangle.

\par
One way to allow this would be to use the process of inheritance available in C++.
Which means, for example, create a class called Shape and create sub classes like Triangle, Plane or Sphere that are derived from that class.
And each of those classes would implement different intersect methods.
In this way, it would be possible for the programmer to develop his own custom shapes as new Rendering Components.
But of course, the benefit of this flexibility also brings the downside of performance loss as the application will have at runtime to figure out which of the derived classes is required to call.
Because, calling a virtual function requires a v-table lookup, and can make it 2-3 times slower than a normal function.

\par
The other way to do it is by developing each shape class without inheritance and being each class independent of each other.
This can have greater performance but the downside is that the programmer cannot add new shapes to the library.
This ray tracer was implemented this way because it is not important to let the user add his own custom shapes as rendering components.
Because with only the triangles it is possible to render many types of custom shapes.
Also, the performance is critical in the intersect method of the shapes because it is a method that can be called millions of times while rendering a scene.

So, all shapes provide three important methods:

\begin{lstlisting}[caption={Main methods in Shape}, captionpos=b, label=Shape]
Intersection intersect(const Intersection &intersection, const Ray &ray) const noexcept;
bool intersect(const AABB &box) const noexcept;
AABB getAABB() const noexcept;
\end{lstlisting}

The first
$intersect$
method determines if a ray intersects the shape and returns the corresponding intersection.
It is important to mention that, obviously, each shape was developed with different algorithms.

The second
$intersect$
method checks if an AABB contains the shape, and only returns true or false.
This is useful to determine in which AABB the shape belongs to in the regular grid accelerator.

Finally, the
$getAABB$
method, as the name implies returns the smallest AABB that surrounds the shape.
This is used in the building of the acceleration structures because it allows to check if a ray intersects multiple shapes inside an AABB.
And it is simpler to intersect an AABB than a shape.

\subsubsection{Plane}

\par
The plane is an essential primitive shape because it allows the user to build indoor scenes.

\par
The construction of a plane requires just an arbitrary point in the plane and the normal of the plane.
So, the intersect method implemented has the following algorithm taken from \cite{RayPlane}:

Let
\begin{equation}
\label{intersectionPlane}
\begin{aligned}
intersectionPoint = rayOrg + rayDir * dist \\
n \cdot (q - x) = 0 \\
n \cdot (q - intersectionPoint) = 0 \\
n \cdot (q - rayOrg - rayDir * dist) = 0 \\
n \cdot (q - rayOrg) = (n \cdot rayDir) * dist \\
dist = [ n \cdot (q - rayOrg) ] / (n \cdot rayDir) \\
\end{aligned}
\end{equation}

where
\begin{conditions*}
	n  &  normal of the plane \\
	q  &  known point on the plane \\
	x  &  any point on the plane \\
	rayOrg & origin of the ray \\
	rayDir & direction of the ray \\
	dist & distance of the ray \\
	intersectionPoint  &  intersection point of the ray in the plane \\
\end{conditions*}


\begin{figure}[H]
	\centering
	\caption{Illustration of a ray intersecting a plane (\cite{PlaneRayIntersection}).}
	\label{Plane.}
	\includegraphics[keepaspectratio,scale=0.4]{RayToPlane.png}
\end{figure}

\par


\subsubsection{Sphere}

\par
The sphere is also an important primitive shape because it allows the user to build some common objects with a shape of a ball.

\par
The construction of a sphere requires just the point in the center and the radius of the sphere.
It also should be noted that a ray can intersect a sphere at two points and therefore it is necessary to determine the closest intersection point to the origin of the ray.
The algorithm used in this library was the algebraic version.
So, the intersect method implemented has the following algorithm taken from \cite{RaySphere}:

Let
\begin{equation}
\label{intersectionSphere}
\begin{aligned}
intersectionPoint = rayOrg + rayDir * dist \\
dist=\frac{-b\pm\sqrt{b^2-4ac}}{2a} \\
\end{aligned}
\end{equation}
where
\begin{conditions*}
	a  &  $\|rayDir\| ^ 2$ \\
	b  &  $2 * rayOrgToCenter \cdot rayDir$ \\
	c  &  $\|rayOrgToCenter\| ^ 2 - sphereRadius ^ 2$ \\
	rayOrgToCenter & vector from the origin point of the ray to the center of the sphere \\
	sphereRadius & radius of the sphere \\
	sphereCenter & center point of the sphere \\
	rayOrg & origin point of the ray \\
	rayDir & direction of the ray \\
	dist & distance of the ray \\
	intersectionPoint  &  intersection point of the ray in the sphere \\
\end{conditions*}

\begin{figure}[H]
	\centering
	\caption{Illustration of a ray intersecting a sphere in two points (\cite{SphereRayIntersection}).}
	\label{Sphere.}
	\includegraphics[keepaspectratio,scale=0.4]{Ray_Sphere_Intersection.png}
\end{figure}

\subsubsection{Triangle}

\par
And finally, obviously the triangle has also been implemented because, as it is the simplest primitive with an area, it allows to build many different object shapes.
The implemented Ray-Triangle Intersection algorithm was the Möller-Trumbore (\cite{RayTriangleIntersection}) algorithm, which was named after its inventors Tomas Möller and Ben Trumbore.

\par
The construction of a triangle requires three points
$[A, B, C]$
, two vectors
$[AB, AC]$
and the normal of the triangle.
This is needed in order to execute the Möller-Trumbore algorithm  for the intersection of a ray with a triangle.
The Möller–Trumbore ray-triangle intersection algorithm is a fast method for calculating the intersection of a ray and a triangle in three dimensions without the needing to precompute the plane equation containing the triangle.
It only needs to store 1 triangle vertex and 2 triangle vectors, which means that each triangle needs only to store 9 floating point values, or 36 bytes.
So, the intersect method implemented has the following algorithm:

Let
\begin{equation}
\begin{aligned}
\label{intersectionTriangle}
intersectionPoint = rayOrg + rayDir * dist \\
dist = (AC \cdot ((rayOrg - A) * AB)) / (AB \cdot (rayDir * AC)) \\
\end{aligned}
\end{equation}

where
\begin{conditions*}
	A & point A of the triangle \\
	AB & vector of the triangle from point A to B \\
	AC & vector of the triangle from point A to C \\
	rayOrg & origin point of the ray \\
	rayDir & direction of the ray \\
	dist & distance of the ray \\
	intersectionPoint  &  intersection point of the ray in the triangle \\
\end{conditions*}

\begin{figure}[H]
	\centering
	\caption{Illustration of a ray intersecting a triangle (\cite{TriangleRayIntersection}).}
	\label{Sphere.}
	\includegraphics[keepaspectratio,scale=1.0]{triangle.png}
\end{figure}

\subsection{Acceleration Structures}

\par
As previously stated, the rendering algorithms based on ray tracing can be very computationally demanding, like Path Tracing and Bidirectional Path Tracing.
These algorithms are very computationally demanding because if the scene is very complex, made with millions of primitives, it is necessary to try to intersect every ray casted into the scene with those millions of primitives.
This task is, obviously, very time consuming and can waste a lot of battery power on the mobile device.
Luckily, there are already known techniques, called acceleration structures, that helps to accelerate this process by reducing the number of intersections.

\par
There are two types of approaches that acceleration structures can have:

\begin{itemize}
	\item Subdivision of Space: the 3D space of the scene is divided into smaller portions of space which each volume element can be uniform or irregular:
	\begin{itemize}
		\item Regular Grids
		\item Octrees
		\item Kd-trees
	\end{itemize}
	\item Subdivision of Objects: the 3D space of the scene is divided by aggregating the 3D primitives in groups which are next to each others:
	\begin{itemize}
		\item Bounding Volume Hierarchy (BVH)
	\end{itemize}
\end{itemize}

\par
The idea in Subdivision of Space is allowing the intersections to start with the nearest primitives of the ray and is only intersected with the further ones, if the closest ones are not in the same direction of the ray.
These structures also allow the rapid and simultaneous rejection of groups of primitives.

\par
In contrast, structures based on Subdivision of objects only allow the rapid and simultaneous rejection of groups of primitives.

\par
The developed library provides only two types of structures, one of each approach: the regular grid and the bounding volume hierarchy.
Both structures are built with Axis Aligned Bounding Volumes (AABB), which are volume elements (voxels) that can cover, one or more primitives from the 3D scene.
An AABB is a voxel, like the name implies, that is aligned to the axis of the scene.
This type of voxels are great to use in acceleration structures because it is very fast to test whether or not a ray intersects that voxel and it only needs to store, in memory, two points of that box.
Besides AABB, it is possible to build the structures with other types of voxels, like:

\begin{itemize}
	\item Sphere: intersection with a sphere is simpler than AABB but covers more 3D space than the scene primitive needs.
	\item Oriented Bounding Volume: its like an AABB but rotated according to the orientation of the primitive.
	\item k-Discrete Oriented Prototype (k-DOP): a generalization of AABBs defined by k hyperplanes with normals in discrete directions.
	\item Convex Hull: is the smallest and more complex convex volume containing the object.
\end{itemize}

\subsubsection{Regular Grid}

\par
A regular grid is an acceleration structure where the scene space is subdivided into equal voxels.
Each voxel is a volume element where it could have inside one or more primitives of the scene, if a ray intersects that voxel, then it tries to intersect the primitives inside it.
The order of testing each voxel is from the nearest of the ray into the furthest in the direction of that ray.
This type of structure is very fast to build but its traversal is poorly efficient due to poor distribution of primitives by voxels.
The figure \ref{RegularGrid.} shows the process of intersecting a ray with the primitives in a scene.

\begin{figure}[H]
	\centering
	\caption{Illustration of traversing a regular grid acceleration structure (\cite{RegularGrid}).}
	\label{RegularGrid.}
	\includegraphics[keepaspectratio,scale=1.0]{Regular_Grid.jpg}
\end{figure}

\par
The developed ray tracer library provides a regular grid as an acceleration structure and its trace operation is as shown in the figure \ref{RegularGrid.}.

\subsubsection{Bounding Volume Hierarchy}

\par
Bounding Volume Hierarchy is another acceleration structure provided in the developed library.
This structure is different from the regular grid because the space is not divided into voxels of the same size.
The space is divided in a way where the primitives of the scene are grouped according to the nearest primitives of each.
This allows to build a structure of voxels where each voxel has, for sure, one or more primitives inside.

\par
The figure \ref{BVH.} shows an example of a BVH structure of a scene.
As it can be seen, each primitive is covered by an AABB, and then two or more AABBs are covered by one larger AABB.
In this library, this is built in a top down manner, where the scene is divided by a fake plane that divides the scene into two AABBs.
The calculation of those planes is performed with the Surface Area Heuristic (SAH) algorithm which is a very popular heuristic commonly used in ray tracers.

\begin{figure}[H]
	\centering
	\caption{Illustration of traversing a Bounding Volume Hierarchy acceleration structure (\cite{BVH}).}
	\label{BVH.}
	\includegraphics[keepaspectratio,scale=1.0]{BVH.jpg}
\end{figure}

\par
In this library, the BVH class is a template, because this way, it is possible to create multiple BVH classes where each one store a vector of primitives with different shapes.

\subsubsection{Surface Area Heuristic}

\par
As it was said above, the library uses the Surface Area Heuristic in order to split the scene.
The algorithm used in the library is very simple, it just needs to calculate the minimum area surface of the sum of left and right sub-trees, as following:

\begin{equation}
SAH_{optimal} = min(S_L * N_L + S_R * N_R)
\end{equation}

\begin{conditions}
	N_L & number of polygons in left subtree \\
	N_R & number of polygons in right subtree \\
	S_L & surface area of left subtree \\
	S_R & surface area of right subtree \\
\end{conditions}

\par
The figure \ref{SAH.} shows a very simple example of how the SAH algorithm works.

\begin{figure}[H]
	\centering
	\caption{Illustration of how Surface Area Heuristic works in Bounding Volume Hierarchy acceleration structure (\cite{SAH}).}
	\label{SAH.}
	\includegraphics[keepaspectratio,scale=1.0]{SAH_example.png}
\end{figure}

\subsection{Primitive}

\par
The Primitive is a template class that needs a Shape and wraps it with the Material together.
It is useful to be a template class because this way it allows to different shapes have different types of material.

\begin{lstlisting}[caption={Main methods in Primitive}, captionpos=b, label=Primitive]
Intersection intersect(Intersection intersection, const Ray &ray) noexcept;
AABB getAABB() const noexcept;
\end{lstlisting}

\par
The \textit{intersect}, as the name implies, is a method that tries to intersect a ray with the primitive.
This method calls the \textit{intersect} of the Shape and it returns the closest intersection to the origin of that ray.
It is used to determine the intersection of the casted ray with the primitives in the scene.

\par
The \textit{getAABB} is the method that returns the AABB voxel that surrounds the primitive as it just returns the call of \textit{getAABB} from the Shape.
It is used to construct the Acceleration Structures.


\section{Rendering Components}

\par
In order to show the functionalities provided by the library, it was developed a few Rendering Components.
It is provided the perspective and orthographic cameras, the area and point lights, six Samplers, five Shaders and one Object loader.

\subsection{Shaders}

\par
The implemented ray tracer was programmed in an objected oriented fashion, so each Rendering Component was developed separately, in a different class.
This allows the user to develop his own Rendering Components without having to develop the ray tracer engine.

\par
A shader is the most important Rendering Component because it is the Rendering Component that describes how the Rendering Equation is approximated.
The Rendering Equation describes how the total radiance reflected by any point
$p$
of a surface in a direction
$\omega$$r$
is calculated.
The Bidirectional Reflectance Distribution Function (BRDF) is a function that tries to approximate the Rendering Equation.
In summary, a shader, in this context, is the algorithm of a BRDF.

\begin{figure}[H]
	\centering
	\caption{Illustration of the rendering equation describing the total amount of light emitted from a point
	$x$
	along a particular viewing direction and given a function for incoming light and a BRDF (\cite{RenderingEquation}).}
	\label{Rendering_Equation.}
	\includegraphics[width=10cm,height=10cm,keepaspectratio,scale=1.0]{Rendering_eq.png}
\end{figure}

\par
There were developed five shaders: NoShadows, Whitted, PathTracer, DepthMap and DiffuseMaterial.
All shaders provide only one main method that allows the user to calculate the RGB color of a pixel with the information of a ray casted into the scene and its nearest calculated intersection.
This method was implemented differently in each shader.
Besides that method, the class Shader provides three other important methods that all shaders have access to:

\begin{lstlisting}[caption={Main methods of Shader}, captionpos=b, label=Shader]
virtual bool shade(glm::vec3 *rgb, const Intersection &intersection, const Ray &ray) noexcept = 0;
glm::vec3 getCosineSampleHemisphere(const glm::vec3 &normal) const noexcept;
bool rayTrace(glm::vec3 *rgb, const Ray &ray) noexcept;
bool shadowTrace(Intersection intersection, const Ray &ray) noexcept;
\end{lstlisting}

\par
The method
$shade$
calculates the color of the pixel that will be stored in the parameter
$rgb$
and the
$intersection$
contains the calculated information about the intersection of a ray with the nearest primitive of the scene like the intersection point, its normal and the material of the intersected material.
Finally the parameter
$ray$
is where the information about the ray like its origin and its direction is accessed.

\par
The method
$getCosineSampleHemisphere$
calculates a random direction in World coordinates around an hemisphere with a given
$normal$.
This method is used to cast new secondary rays in the scene in order to simulate the indirect light.
This is typically used in Path Tracers.

\subsubsection{DepthMap}

\par
DepthMap is a very simple shader that calculates an image or image channel that contains information relating to the distance of the surfaces of scene objects from a viewpoint (\cite{DepthMap}).
It is useful to various things like simulating the effect of uniformly dense semi-transparent media within a scene, such as fog, smoke or large volumes of water; or even to create Shadow Maps.

The algorithm is as much as simple as:

\begin{algorithm}
	\small
	\label{DepthMap}
	\caption{Algorithm of DepthMap Shader}
	\Input{\var{maxPoint}, \var{rayOrigin},\var{intersectionLength}}
	\Output{\var{outputRGB}}
	
	$\var{maxDist} \gets ||\var{maxPoint} - \var{rayOrigin}||$ \\
	$\var{depth} \gets (\var{maxDist} - \var{intersectionLength}) / \var{maxDist}$ \\
	$\var{outputRGB} \gets [\var{depth}, \var{depth}, \var{depth}]$ \\
\end{algorithm}

%where
%\begin{conditions*}
%	maxPoint & is the furthest 3D point that the user send as parameter in the DepthMap constructor \\
%	rayOrigin & origin point of the ray \\
%	intersectionLength & the distance from the origin of the ray to the intersection point  \\
%	outputRGB & the output parameter to write the pixel's RGB color \\
%\end{conditions*}

%Where the
%$maxPoint$
%is the furthest 3D point that the user send as parameter in the DepthMap constructor.

\begin{figure}[H]
	\centering
	\caption{DepthMap shader.}
	\label{DepthMap shader.}
	\includegraphics[keepaspectratio,scale=0.3]{DepthMap.png}
\end{figure}


\subsubsection{DiffuseMaterial}

\par
The DiffuseMaterial is another very simple shader that only computes directly the diffuse material color of the intersected primitive.
But, in case the primitive material does not have diffuse color, the specular color is computed or the transmission color or even the light emission.
This is can be useful to debug the application because it renders the scene, including all types of materials, very fast.
And the algorithm is very simple as illustrated below:

\begin{algorithm}
	\small
	\label{DiffuseMaterial}
	\caption{Algorithm of DiffuseMaterial Shader}
	\Input{\var{kD}, \var{kS}, \var{kT}, \var{Le}}
	\Output{\var{outputRGB}}
	
	\eIf{\var{kD} > $0$}{
		$\var{outputRGB} \gets \var{kD}$ \\
	}{\eIf{\var{kS} > $0$}{
			$\var{outputRGB} \gets \var{kS}$ \\
		}{\eIf{\var{kS} > $0$}{
				$\var{outputRGB} \gets \var{kT}$ \\
			}{\uIf{\var{Le} > $0$}{
					$\var{outputRGB} \gets \var{Le}$ \\
				}
			}
		}
	}
\end{algorithm}

\begin{figure}[H]
	\centering
	\caption{DiffuseMaterial shader.}
	\label{DiffuseMaterial shader.}
	\includegraphics[keepaspectratio,scale=0.3]{DiffuseMaterial.png}
\end{figure}


\subsubsection{NoShadows}

\par
NoShadows is a simple shader because, as the name implies, it does not synthesize the shadows and it only simulates the direct lighting.
This BRDF only simulates direct lighting in primitives with diffuse surfaces and it does not take into account the light coming from other points.
As already said, the indirect lighting is not fully simulated but rather approximated with a fixed ambient light of about 10\% of the diffuse color of the objects.
This can be useful in order to let the user render a scene with good details and without having to wait as long as the Whitted algorithm.

\par
The algorithm developed is as following:

\begin{algorithm}
	\small
	\label{NoShadows}
	\caption{Algorithm of NoShadows Shader}
	\Input{\var{intersectionPoint}, \var{intersectionNormal}}
	\Output{\var{outputRGB}}
	
	\uIf{intersected material is diffuse}{
		\ForEach{light}{
			\ForEach{sample}{
				$\vec{vectorToLight} \gets \var{lightPos} - \var{intersectionPoint}$ \\
				$\var{cosNL} \gets \vec{vectorToLight} \cdot \vec{intersectionNormal}$ \\
				\uIf{\var{cosNL} > $0$}{
					$\var{outputRGB} \gets \var{outputRGB} + (\var{kD} * \var{radLight} * \var{cosNL})$ \\
				}
			}
		}
		$\var{outputRGB} \gets \var{outputRGB} / \var{\#samples}$ \\
		$\var{outputRGB} \gets \var{outputRGB} / \var{\#lights}$ \\
		$\var{outputRGB} \gets \var{outputRGB} + \var{kD} * 0.1$ \\
	}
\end{algorithm}

\begin{figure}[H]
	\centering
	\caption{NoShadows shader.}
	\label{NoShadows shader.}
	\includegraphics[keepaspectratio,scale=0.3]{NoShadows.png}
\end{figure}

\subsubsection{Whitted}

\par
As the name of this shader implies, Whitted is the algorithm presented by John Turner Whitted in the 1980s.
Like the previous shader, it doesn't simulate indirect lighting.
This BRDF recursively perform the ray casting to simulate refraction and reflection on diffuse and specular surfaces.
In order to simulate a reflective and refractive surfaces, the algorithm was divided into each case.
This makes it possible to simulate refractive surfaces, reflective surfaces and even refractive and reflective surfaces.

\par
The reflected light in a diffuse surface is calculated by adding the casting rays in direction to the lights.
These rays are called shadow rays and their radiance are multiplied by the dot product between vector to the light and the shading normal.
At the end, the summation is multiplied by the diffuse color of the intersected primitive and divided by the number of samples taken.

\par
The reflected light in a specular surface is calculated in a different way.
The specular ray is casted and ray traced, then the obtained radiance is multiplied by the specular color of the intersected primitive.
The reflection direction is the subtraction between the double of dot product between the inverse of the ray direction and the shading normal multiplying by the shading normal, and the inverse of the ray direction.

\begin{figure}[H]
	\centering
	\caption{Reflections projected on the floor and on the spheres (\cite{RTReflections}).}
	\label{Reflection.}
	\includegraphics[keepaspectratio,scale=0.3]{Reflection.png}
\end{figure}

\par
Finally, the refracted light in a transmission surface is calculated by using the refractive index of the intersected primitive and the ray direction and shading normal.

The specular refraction is calculated as follows:

\begin{algorithm}
	\small
	\label{Transmission}
	\caption{Algorithm of Specular Transmission}
	\Input{\var{rayDir}, \var{intersectionNormal}, \var{refractiveIndice}, \var{rayDepth}, \var{intersectionPoint}, \var{intersectionPrimitive},  \var{kT}}
	\Output{\var{specularTransmissionColor}}
	
	$\vec{refractDir} \gets glm::refract(\vec{rayDir}, \vec{intersectionNormal}, 1 / \var{refractiveIndice})$ \\
	$\var{transmissionRay} \gets Ray (\vec{refractDir}, \var{intersectionPoint}, \var{rayDepth} + 1, \var{intersectionPrimitive})$ \\
	$\var{LiT\_RGB} \gets rayTrace (\var{transmissionRay})$ \\
	$\var{specularTransmissionColor} \gets \var{kT} * \var{LiT\_RGB}$ \\
\end{algorithm}

\iffalse
First, the shading normal is inverted if the origin of the ray was inside a primitive, like a sphere, and if it was not then the refractive index is inverted.
Then, it is calculated two auxiliary scalar projections:
$cosTheta1$
and
$cosTheta2$
.
$cosTheta1$
is the dot product of the inverse of the shading normal and the ray direction, and
$cosTheta2$
is the difference of one and refractive index squared multiplied by one minus
$cosTheta1$
squared.
Then, if
$cosTheta2$
is greater than zero, then the direction of the transmission ray is ray direction multiplied with refractive index plus shading normal multiplied by refractive index multiplied $cosTheta1$
minus square root of 
$cosTheta2$
.
Else, the direction of the transmission ray is just ray direction plus shading normal multiplied by the double of
$cosTheta1$
.
\fi

And, as usual, the transmission ray is traced and its radiance is multiplied by the transmission component of the intersected primitive.

\begin{figure}[H]
	\centering
	\caption{Refraction of light at the interface of two media of different refractive indices (\cite{RTRefractions}).}
	\label{Refraction.}
	\includegraphics[keepaspectratio,scale=0.3]{Refraction.png}
\end{figure}

\par
The algorithm developed is as following:

\begin{algorithm}
	\small
	\label{Whitted}
	\caption{Algorithm of Whitted Shader}
	\Input{\var{rayDirection}, \var{intersectionNormal}, \var{refractiveIndice}, \var{rayDepth}, \var{intersectionPoint}, \var{intersectionPrimitive},  \var{kT}, \var{kS}}
	\Output{\var{outputRGB}}
	
	\uIf{intersected material is diffuse} {
		\ForEach{light}{
			\ForEach{sample}{
				$\vec{vectorToLight} \gets \var{lightPos} - \var{intersectionPoint}$ \\
				$\var{cosNL} \gets \vec{vectorToLight} \cdot \vec{intersectionNormal}$ \\
				\uIf{\var{cosNL} > $0$}{
					$\var{shadowRay} \gets Ray(\var{intersectionPoint}, \vec{vectorToLight}, \var{distanceToLight}, \var{rayDepth} + 1)$ \\
					\uIf{!shadowTrace (\var{shadowRay})}{
						$\var{outputRGB} \gets \var{outputRGB} + \var{radLight} * \var{cosNL}$ \\
					}
				}
			}
		}
		$\var{outputRGB} \gets \var{outputRGB} * \var{kD}$ \\
		$\var{outputRGB} \gets \var{outputRGB} / \var{\#samples}$ \\
	}

	\uIf{intersected material is specular reflective} {
		$\vec{reflectionDir} \gets glm::reflect(\vec{rayDir}, \vec{intersectionNormal})$ \\
		$\var{specularRay} \gets Ray(\vec{reflectionDir}, \var{intersectionPoint}, \var{rayDepth} + 1, \var{intersectionPrimitive})$ \\
		$\var{LiS\_RGB} \gets rayTrace(\var{specularRay})$ \\
		$\var{outputRGB} \gets \var{outputRGB} + \var{kS} * \var{LiS\_RGB})$ \\
	}
	
	\uIf{intersected material is specular refractive} {
		$\vec{refractDir} \gets glm::refract(\vec{rayDir}, \vec{intersectionNormal}, 1 / \var{refractiveIndice})$ \\
		$\var{transmissionRay} \gets Ray (\vec{refractDir}, \var{intersectionPoint}, \var{rayDepth} + 1, \var{intersectionPrimitive})$ \\
		$\var{LiT\_RGB} \gets rayTrace (\var{transmissionRay})$ \\
		$\var{outputRGB} \gets \var{outputRGB} + \var{kT} * \var{LiT\_RGB}$ \\
	}
\end{algorithm}

\begin{figure}[H]
	\centering
	\caption{Whitted shader.}
	\label{Whitted shader.}
	\includegraphics[keepaspectratio,scale=0.3]{Whitted.png}
\end{figure}


\subsubsection{PathTracer}

\par
Finally, the last developed shader is the canonical Path Tracer which is a Monte Carlo method of rendering images of three-dimensional scenes such that the global illumination is faithful to reality.
This renderer algorithm can synthesize very realistic scenes.
Unlike the previous shaders, this one fully simulates both direct and indirect lighting.
But, like the previous shader, this algorithm simulates the light reflected on diffuse and specular surfaces, as well as the light refracted on transparent primitives.

\par
The light reflected on diffuse surfaces is divided in two parts: direct lighting and indirect lighting.
The direct lighting is calculated in a similar way to the Whitted shader but with the difference that the samples are not taken from all sources of light but rather only one.
The light source in each sample is chosen randomly.
Then the obtained light radiance is multiplied by the Probability Density Function (PDF) in order to approximate the expected value.
In this case the PDF is as simple as
$1 / \#lights$.

\par
Finally, the indirect lighting reflected in diffuse surfaces is calculated in a much more complex way.
First it is generated, from the intersected point, a random direction on an unit hemisphere with a PDF proportional to cosine-weighted solid angle\\
($PDF: p(\Theta) = cos\theta / \pi$,
$x = cos(2\pi r1)\sqrt{1-r2}$,
$y = sin(2\pi r1)\sqrt{1-r2}$,
$z = \sqrt{r2}$)
.
This kind of PDF is one of the best ways to render images with path tracing that the rendering equation converges to the expected value as fast as possible.

Then to generate a new ray direction in a world coordinates from a hemisphere it is needed to distinguish the six different possible materials:

\begin{itemize}
	\item Diffuse reflection
	\item Glossy reflection
	\item Specular reflection
	\item Diffuse transmission
	\item Glossy transmission
	\item Specular transmission
\end{itemize}

\par
The figure \ref{Secondary_Rays.} illustrates those six possible scenarios.

To calculate the secondary ray direction in a diffuse reflective material, it was implemented a generator of random directions on unit hemisphere proportional to cosine-weighted solid angle around the normal of that hemisphere.
Sampling in a cosine weighted hemisphere is better than uniform sampling over the hemisphere about the normal because the rays casted at the bottom of the hemisphere will not contribute as much as the other directions because of the multiplication by the cosine of theta.

\par
To do this, it was implemented a method called \textit{getCosineSampleHemisphere}, which generates a random direction around an hemisphere oriented by an arbitrary normal.

\begin{lstlisting}[caption={Method in Shader which generates a random direction in an hemisphere.}, captionpos=b, label=Shader]
glm::vec3 getCosineSampleHemisphere(glm::vec3 normal);
\end{lstlisting}

\par
The algorithm to generate a random direction in an hemisphere was taken from  \cite{CosineSampleHemisphere}, which is as follows:

\begin{algorithm}
	\small
	\label{Shader}
	\caption{Method in Shader which generates a random direction in an hemisphere.}
	\Input{$\vec{normal}$}
	\Output{$\vec{newDir}$}
	
	$\var{phi} \gets 2 * \pi * \var{uniformRandom1}$ \\
	$\var{r2} \gets \var{uniformRandom2}$ \\
	$\var{cosTheta} \gets \sqrt{\var{r2}}$ \\
	$\vec{u} \gets \left| \var{normal.x} \right| > 0.1? [0,1,0] : [1,0,0]$ \\
	$\vec{u} \gets normalize(\vec{u} \times \vec{normal})$ \\
	$\vec{v} \gets \vec{normal} \times \vec{u}$ \\
	$\vec{newDir} \gets normalize(\vec{u} * (\cos(\var{phi}) * \var{cosTheta}) + \vec{v} * (\sin(\var{phi}) * \var{cosTheta}) + \vec{normal} * \sqrt(1 - \var{r2})) $ \\
\end{algorithm}

The figure \ref{Spherical_Coordinates.} shows the spherical coordinate system with the respective polar angle
$\theta$
(theta) and the azimuthal angle
$\phi$
(phi).

And the indirect lighting on diffuse reflective surfaces is then the multiplication of the ray traced light radiance with the material diffuse color of the primitive and
$\pi$
, and divided by the probability of continuing the Russian roulette.
The set of equations \ref{Diffuse_Reflection.} shows the simplification of the reflected diffuse color equation.

\begin{MyEquation}[H]
\caption{The resolution of diffuse reflected color of an object.}
\begin{equation}
\begin{gathered}
\label{Diffuse_Reflection.}
cos (\theta) = cos(\overrightarrow{NewDirection}, \overrightarrow{N})\\
\\
PDF = cos(\theta) / \pi\\
PDF = cos(\overrightarrow{NewDirection}, \overrightarrow{N}) / \pi\\
\\
LiD += kD * RGB_{SecondaryRay} * cos (\overrightarrow{NewDirection}, \overrightarrow{N}) / (PDF * ContinueProbability)\\
LiD += kD * RGB_{SecondaryRay} * \pi / ContinueProbability
\end{gathered}
\end{equation}
\end{MyEquation}

\begin{figure}[H]
	\centering
	\caption{The spherical coordinates (\cite{SphericalCoordinates}).}
	\label{Spherical_Coordinates.}
	\includegraphics[keepaspectratio,scale=0.5]{Spherical_Coordinates.png}
\end{figure}

\begin{figure}[H]
	\centering
	\caption{The possible directions in secondary rays (\cite{SamplingRayDirection}).}
	\label{Secondary_Rays.}
	\includegraphics[keepaspectratio,scale=0.5]{SecondaryRayDirection.png}
\end{figure}

\par
The reflected and refracted light in a specular surface are simulated in a similar way to the Whitted algorithm presented previously.

\par
So, the algorithm developed in Path Tracer Shader is as following:

\begin{algorithm}
	\small
	\label{PathTracer}
	\caption{Algorithm of Path Tracer Shader}
	\Input{\var{rayDir}, \var{intersectionNormal}, \var{refractiveIndice}, \var{rayDepth}, \var{intersectionPoint}, \var{intersectionPrimitive}, \var{kD}, \var{kT}, \var{kS}}
	\Output{\var{outputRGB}}
	
	\uIf{intersected material is diffuse}{
		\ForEach{light}{
			\ForEach{sample}{
				$\var{lightPos} \gets \var{samplerLight}.getSample()$
				$\vec{vectorToLight} \gets \var{lightPos} - \var{intersectionPoint}$ \\
				$\var{cosNL} \gets \vec{vectorToLight} \cdot \vec{intersectionNormal}$ \\
				\uIf{$\var{cosNL} > 0$} {
					$\vec{shadowRay} \gets Ray(\var{intersectionPoint}, \vec{vectorToLight}, \var{distanceToLight}, \var{rayDepth} + 1)$ \\
					\uIf{$!shadowTrace(\var{shadowRay})$} {
						$\var{Ld} \gets \var{Ld} + \var{radLight} * \var{cosNL}$ \\
					}
				}
			}
		}
		$\var{Ld} \gets \var{Ld} * \var{kD}$ \\
		$\var{Ld} \gets \var{Ld} * \var{\#lights}$ \\
		$\var{Ld} \gets \var{Ld} / \var{\#samples}$ \\
		
		\uIf{\var{rayDepth} <= RAY\_DEPTH\_MIN || uniform\_dist(gen) > finish\_probability} {
			$\vec{newDir} \gets getCosineSampleHemisphere (\vec{intersectionNormal})$ \\
			$\var{secundaryRay} \gets Ray(\vec{newDir}, \var{intersectionPoint}, \var{rayDepth} + 1, \var{intersectionPrimitive})$ \\
			$\var{LiD\_RGB} \gets rayTrace(\var{secundaryRay})$ \\
			$\var{LiD} \gets \var{kD} * \var{LiD\_RGB}$ \\
			\uIf{rayDepth > RAY\_DEPTH\_MIN} {
				$\var{LiD} \gets \var{LiD} / \var{continue\_probability}$ \\
			}
			\uIf{\var{Ld} > 0} {
				$\var{LiD} \gets 0$ \\
			}
		}
	}
		
	\uIf{intersected material is specular reflective} {
		$\vec{reflectionDir} \gets glm::reflect(\vec{rayDir}, \vec{intersectionNormal})$ \\
		$\var{specularRay} \gets Ray(\vec{reflectionDir}, \var{intersectionPoint}, \var{rayDepth} + 1, \var{intersectionPrimitive})$ \\
		$\var{LiS\_RGB} \gets rayTrace(\var{specularRay})$ \\
		$\var{LiS} \gets \var{kS} * \var{LiS\_RGB}$ \\
	}
	
	\uIf{intersected material is specular refractive} {
		$\vec{refractDir} \gets glm::refract(\vec{rayDir}, \vec{intersectionNormal}, 1 / \var{refractiveIndice})$ \\
		$\var{transmissionRay} \gets Ray (\vec{refractDir}, \var{intersectionPoint}, \var{rayDepth} + 1, \var{intersectionPrimitive})$ \\
		$\var{LiT\_RGB} \gets rayTrace (\var{transmissionRay})$ \\
		$\var{LiT} \gets \var{kT} * \var{LiT\_RGB}$ \\
	}

	$\var{outputRGB} \gets \var{outputRGB} + \var{Ld} + \var{LiD} + \var{LiS} + \var{LiT}$ \\
\end{algorithm}

\begin{figure}[H]
	\centering
	\caption{PathTracer shader.}
	\label{PathTracer shader.}
	\includegraphics[keepaspectratio,scale=0.3]{PathTracer.png}
\end{figure}


\subsection{Samplers}

\par
There were implemented four samplers: Constant, Stratified, HaltonSequence and MersenneTwister.

\par
All samplers just provide one method for the user to use:

\begin{lstlisting}[caption={Main methods of Sampler}, captionpos=b, label=Sampler]
float getSample(unsigned sampleNumber);
\end{lstlisting}

\par
The
$getSample$
method receives as parameter the number of the current sample and returns the the actual sample.
An atomic variable
$sample$
is used to count the current index of the samples.

\subsubsection{Constant}

\par
This sampler is the simplest because it always returns the same number passed to the constructor.

\par
This is used when the user only needs one sample per pixel and want all the samples to be in the middle of each pixel.

\par
So, the algorithm of the
$getSample$
is just:

\begin{lstlisting}[caption={Algorithm of Constant Sampler}, captionpos=b, label=Constant]
return value
\end{lstlisting}


\subsubsection{Stratified}

\par
This sampler makes each sample at equal distance
$(1 / domainSize)$
and in ascending order.
For example, for a domain size of 4, the samples taken are going to be: 0, 0.25, 0.5 and 0.75.

\par
This is useful for taking samples when supersampling without any noise, because each sample is not randomly chosen.

\par
The algorithm of the
$getSample$
is then:

\begin{lstlisting}[caption={Algorithm of Stratified Sampler}, captionpos=b, label=Stratified]
current = sample++ //atomic operation
if (current >= (domainSize * (sampleNumber + 1))) {
  sample--
  return 1
}
unsigned task = current - (sampleNumber * domainSize)
resSample = task / domainSize
return resSample
\end{lstlisting}

\begin{algorithm}
	\small
	\label{Stratified}
	\caption{Algorithm of Stratified Sampler}
	\Input{\var{sampleNumber}}
	\Output{\var{resSample}}
	
	$\var{sample} \gets \var{sample} + 1$ //atomic operation \\
	$\var{current} \gets \var{sample}$ \\
	\eIf{current >= (domainSize * (sampleNumber + 1))}{
		$\var{sample} \gets \var{sample} - 1$ //atomic operation \\
		$\var{resSample} \gets 1$ \\
	} {
		$\var{task} \gets \var{current} - (\var{sampleNumber} * \var{domainSize})$ \\
		$\var{resSample} \gets \var{task} * \var{domainSize}$ \\
	}
\end{algorithm}

This algorithm ensures that if the current sample is greater than the domain size, then it will always return 1 which means the end of the sampler.


\subsubsection{HaltonSequence}

\par
As the name implies, this sampler generates the Halton sequence.

\par
Halton sequence is a quasi random number sequence which is a deterministic sequence with low discrepancy.
These sequences are usually good for Rendering Algorithms like Path Tracing because it can make the rendering equation converge faster, that is, with fewer samples.

\begin{figure}[H]
	\centering
	\caption{Halton sequence in a 2D image plane (\cite{HaltonSequence}).}
	\label{Halton_sequence_2D.}
	\includegraphics[keepaspectratio,scale=0.5]{Halton_sequence_2D.png}
\end{figure}

\par
The algorithm of the
$getSample$
is as following:

\begin{lstlisting}[caption={Algorithm of HaltonSequence Sampler}, captionpos=b, label=HaltonSequenceSampler]
current = sample++ //atomic operation
if (current >= (domainSize * (sampleNumber + 1))) {
  sample--
  return 1
}
unsigned task = current - (sampleNumber * domainSize)
float resSample = MobileRT::haltonSequence(task, 2)
return resSample
\end{lstlisting}

\begin{algorithm}
	\small
	\label{HaltonSequenceSampler}
	\caption{Algorithm of HaltonSequence Sampler}
	\Input{\var{sampleNumber}}
	\Output{\var{resSample}}
	
	$\var{sample} \gets \var{sample} + 1$ //atomic operation \\
	$\var{current} \gets \var{sample}$ \\
	\eIf{current >= (domainSize * (sampleNumber + 1))}{
		$\var{sample} \gets \var{sample} - 1$ //atomic operation \\
		$\var{resSample} \gets 1$ \\
	} {
		$\var{task} \gets \var{current} - (\var{sampleNumber} * \var{domainSize})$ \\
		$\var{resSample} \gets haltonSequence(\var{task}, 2)$ \\
	}
\end{algorithm}

\par
As it can be seen in listing \ref{HaltonSequenceSampler}, most of the Halton Sequence Sampler is similar to the Stratified Sampler.
The only difference is the call to the \textit{haltonSequence} method where it computes the Halton sequence for an arbitrary index and base number.

\par
The Halton sequence algorithm is very simple as shown in listing \ref{HaltonSequence} and it was taken from \cite{HaltonSequence}.

\begin{lstlisting}[caption={Developed method of HaltonSequence}, captionpos=b, label=HaltonSequence]
float haltonSequence(unsigned index, unsigned base) {
	float f = 1.0
	float result = 0.0
	while (index > 0) {
		f /= base
		result += f * (index % base)
		index = std::floor(index / base)
	}
	return result;
}
\end{lstlisting}

\begin{algorithm}
	\small
	\label{HaltonSequence}
	\caption{Developed method of HaltonSequence}
	\Input{\var{index}, \var{base}}
	\Output{\var{result}}
	
	$\var{f} \gets 1$ \\
	$\var{result} \gets 0$ \\
	\While{$\var{index} > 0$}{
		$\var{f} \gets \var{f} \div \var{base}$ \\
		$\var{result} \gets \var{result} + \var{f} \times (\var{index} \mod \var{base})$ \\
		$\var{index} \gets \floor{\var{index} \div \var{base}}$ \\
	}
\end{algorithm}

\subsubsection{MersenneTwister}

\par
This sampler is just a wrapper to call the constructor of std::uniform\_real\_distribution() and the constructor of std::random\_device() of the standard C++ library.

This allow to construct a random number generator which calculates a random number between 0 and 1 with an uniform distribution.
This generator is a pseudo random number generator (PRNG), which is an algorithm for generating a sequence of numbers whose properties approximate the properties of sequences of random numbers.
Although the PRNG-generated sequence is not truly random, because it is completely determined by an initial value, called the PRNG's seed, it commonly used a std::random\_device for the generator seed because it produces non-deterministic random numbers, when supported.
These sequences are usually used in simulations that use Monte Carlo methods like the Monte Carlo Ray Tracing.

\par
It is a good PRNG because it produces uniformly distributed numbers, it doesn't repeat the same sequence and it does not exhibit correlation between successive numbers.

\begin{figure}[H]
	\centering
	\caption{Pseudorandom sequence in a 2D image plane (\cite{HaltonSequence}).}
	\label{Pseudorandom_sequence_2D.}
	\includegraphics[keepaspectratio,scale=0.5]{Pseudorandom_sequence_2D.png}
\end{figure}

\par
So the algorithm of the
$getSample$
is just:

\begin{lstlisting}[caption={Algorithm of MersenneTwister Sampler}, captionpos=b, label=MersenneTwister]
static std::uniform_real_distribution<float> uniform_dist {0.0, 1.0}
static std::mt19937 gen(std::random_device {}())
float res = uniform_dist(gen)
return res
\end{lstlisting}

\par
As can be seen in listing \ref{MersenneTwister}, the algorithm is very simple because it just constructs two objects in the first time the method is called:

\begin{enumerate}
	\item std::uniform\_real\_distribution (paramA, paramB) - which produces random floating-point values, uniformly distributed on the interval [paramA, paramB), that the user specifies.
	\item std::mt19937 (std::random\_device {}()) - pseudo-random generator of 32-bit numbers with a state size of 19937 bits.
\end{enumerate}

\par
Then every time the user calls \textit{getSample}, the MersenneTwister Sampler just calls the operator() of std::uniform\_real\_distribution generator, which generates the next random number in the distribution.

\subsection{Lights}

\par
There were implemented two types of light sources: point light and area light.
These light sources provide the user two main methods:

\begin{lstlisting}[caption={Main methods in Light}, captionpos=b, label=Light]
glm::vec3 getPosition()
Intersection intersect(Intersection intersection, Ray ray)
\end{lstlisting}

\par
The
$getPosition$
method just returns the position of the light source and the 
$intersect$
method determines whether a given
$ray$
as a parameter intersects this light source and, if it intersects, writes the result to the
$intersection$
parameter.

\subsubsection{Point light}

\par
The Point light is the simplest form of a light because, as the name implies, it is just a point of light which emits light in all directions at once.
This type of light is useful to render fast shadows because it just simulates hard shadows, in that each shadow is only constituted with an umbra, which is the fully dark shadow.

\begin{figure}[H]
	\centering
	\caption{Hard shadows (\cite{Shadows}).}
	\label{Shadows_Hard.}
	\includegraphics[keepaspectratio,scale=0.5]{Shadows_Hard.png}
\end{figure}

\par
Therefore, the
$getPosition$
method is very simple because it only returns the position of the light source determined in its constructor:

\begin{lstlisting}[caption={Algorithm of getPosition in Point light}, captionpos=b, label=getPositionPointLight]
return lightPosition
\end{lstlisting}

\par
And the
$intersect$
method is also quite simple because the ray parameter never intersects the point light.
This is because the probability of a ray intersecting a single 3D point in the world is practically null.

\begin{lstlisting}[caption={Algorithm of intersect in Point light}, captionpos=b, label=intersectPointLight]
return intersection
\end{lstlisting}

\subsubsection{Area light}

\par
The Area light is another type of light source where the light comes from an area.
It can have many different shapes, like square, triangle, circle, etc., and this allows the simulation of soft shadows like the figure \ref{Shadows_Soft.}.

\begin{figure}[H]
	\centering
	\caption{Soft shadows (\cite{Shadows}).}
	\label{Shadows_Soft.}
	\includegraphics[keepaspectratio,scale=0.5]{Shadows_Soft.png}
\end{figure}

\par
The Area light implemented for this application has a shape of a triangle.
This shape is intended as the triangle is the simplest shape which allows to construct more complex shapes.
And, with barycentric coordinates, it is very easy to generate a random point in the triangle.

\par
A triangle with three points: A, B and C.
We get vectors AB and AC, being:

\begin{lstlisting}[caption={Vectors AB and AC in a triangle}, captionpos=b, label=AreaLight]
AB=[Bx-Ax,By-Ay,Bz-Az] and AC=[Cx-Ax,Cy-Ay,Cz-Az].
\end{lstlisting}

\par
These vectors tell us how to get from point A to the other two points in the triangle, by telling us what direction to go and how far.
So, with barycentric coordinates R=1/3, S=1/3 and T=1/3, we get the point in the center of the triangle.
To generate a random point in the triangle, we have to generate two random numbers between 0 and 1 (R and S).
Then we have to make sure we stay inside the triangle by checking if they are larger than one:

\begin{lstlisting}[caption={Algorithm of Area light}, captionpos=b, label=AreaLight]
if (R + S >= 1) {
  R = 1 - R
  S = 1 - S
}
\end{lstlisting}

Finally we can obtain a random point in the triangle by starting at point A, then getting a random percentage along vector AB and a random percentage along vector AC:

\begin{lstlisting}[caption={Algorithm of Area light}, captionpos=b, label=AreaLight]
RandomPointPosition = A + R*AB + S*AC
\end{lstlisting}

\begin{figure}[H]
	\centering
	\caption{Calculation of point P by using barycentric coordinates starting at point A and adding a vector AB and a vector AC (\cite{TriangleBarycentricCoordinates}).}
	\label{Barycentric_Coordinates.}
	\includegraphics[keepaspectratio,scale=0.5]{barycentric-coordinates.png}
\end{figure}

\par
So, the
$getPosition$
algorithm is as following:

\begin{lstlisting}[caption={Algorithm of getPosition in Area light}, captionpos=b, label=AreaLight]
float R = samplerPointLight->getSample()
float S = samplerPointLight->getSample()
if (R + S >= 1.0) {
	R = 1.0 - R;
	S = 1.0 - S;
}
glm::vec3 position = triangle.pointA + R * triangle.AB + S * triangle.AC
return position
\end{lstlisting}

\par
And the
$intersect$
method just calls the intersection of a ray with a triangle method presented earlier, and if it intersects updates the material.

\begin{lstlisting}[caption={Algorithm of intersect in Area light}, captionpos=b, label=AreaLight]
float lastDist = intersection.length
intersection = triangle.intersect(intersection, ray)
intersection.material_= intersection.length < lastDist? &radiance : intersection.material
return intersection
\end{lstlisting}

\subsection{Cameras}

\par
Only two types of cameras were implemented: perspective and orthographic cameras.

\par
The camera only provides one method for the user to cast a ray from the camera position in direction to the image plane:

\begin{lstlisting}[caption={Main methods in Camera}, captionpos=b, label=Camera]
Ray generateRay(float u, float v, float deviationU, float deviationV);
\end{lstlisting}

\par
The method
$generateRay$
needs four parameters to create a ray.
The $u$ and $v$
are used to choose the targeted pixel in the image plane.
Being
$u$
the inverse of the index of the pixel in its line, that is,
$x / width$
, and
$v$
the inverse of the index of the pixel in its column, that is,
$y / height$.
In order to allow the reduction of aliasing in the generated images of the scene, the camera also accepts two extra parameters
$deviationU$ and $deviationV$
which are variances inside a pixel.
The
$deviationU$
is a horizontal variance of the pixel, that is,\\
$[-0.5*pixelWidth, 0.5*pixelWidth]$
and
$deviationV$ the variance in the vertical of the pixel\\
$[-0.5*pixelHeight, 0.5*pixelHeight]$.
This technique is called jittering, as shown in figure \ref{Jittering.}, and allows to reduce the aliasing effect by introducing noise into the output image.
Although the final image gets some noise, this effect turns out to be visually more appealing than aliasing because it is an effect without patterns that are easily detectable by the human eye.
It is important to reduce the aliasing effect because, it is an effect that the human eyes can detect very fast because the generated image will have quite regular patterns, as shown in figure \ref{Aliased.}.

\begin{figure}[H]
	\centering
	\caption{Illustration of how sampling the plane image with jittering works (\cite{Jittering}).}
	\label{Jittering.}
	\includegraphics[keepaspectratio,scale=1.0]{Jittering.png}
\end{figure}

\begin{figure}[H]
	\centering
	\caption{Aliasing effect (\cite{Aliasing}).}
	\label{Aliased.}
	\includegraphics[keepaspectratio,scale=0.6]{aliasing.jpg}
\end{figure}


\par
Sampling with jittering is, as previously stated, a good technique to avoid aliasing in the image.
The figure \ref{Sampling_Stratified.} shows an example of stratified sampling with 1 and 256 samples per pixel.
As can be seen in the figure on the left, there is a noticeably aliasing in the image, and we can't even perceive the correct positions of the black squares on the background.
Of course, with 256 samples per pixel, the aliasing effect is greatly reduced, but also, the execution time is linearly increased, which is a downside to the user experience.

\begin{figure}[H]
	\centering
	\subfloat[Stratified sampling with 1, sample per pixel.][Stratified sampling with 1 \\sample per pixel.]{{\includegraphics[width=5cm]{regular_1spp.png} }}
	\qquad
	\subfloat[Stratified sampling with 256, samples per pixel.][Stratified sampling with 256 \\samples per pixel.]{{\includegraphics[width=5cm]{regular_256spp.png} }}
	\caption{Stratified sampling.}
	\label{Sampling_Stratified.}
\end{figure}

\par
Sampling with jittering is, obviously not perfect, as shown in the figure \ref{Sampling_Jittering.}.
But, it produces more visually appealing images, although it introduces some noise.
Even, with just 1 sample per pixel, the generated image is less "jaggy" in the background.
And, with 4 samples per pixel, the noise is greatly reduced and its quality is visually comparable to the stratified sampling with 256 samples per pixel.
This means that by using jittering it is possible to obtain images pleasing to the human eye with fewer samples per pixel, and consequently in less execution time.

\begin{figure}[H]
	\centering
	\subfloat[Jittered sampling with 1, sample per pixel.][Jittered sampling with 1 \\sample per pixel.]{{\includegraphics[width=5cm]{jittering_1spp.png} }}
	\qquad
	\subfloat[Jittered sampling with 4, samples per pixel.][Jittered sampling with 4 \\samples per pixel.]{{\includegraphics[width=5cm]{jittering_4spp.png} }}
	\caption{Jittered sampling.}
	\label{Sampling_Jittering.}
\end{figure}


\subsubsection{Perspective Camera}

\par
This type of camera is the most used in renderers because it uses perspective projection.
With it, it can simulate images being seen by the human eyes, which means, it simulates the depth of the objects, and produces 2D images with a 3D projection.

\par
To obtain an image plane with perspective, it is necessary to have a Field of View.
In order to accept any resolution of the image plane, we have to divide the field of view in two parts: horizontal and vertical.
This way, we can obtain the aspect ratio of the image plane we want.

\begin{figure}[H]
	\centering
	\caption{Perspective camera with hFov and vFov (\cite{Camera_Perspective_Orthographic}).}
	\label{Perspective_Camera.}
	\includegraphics[keepaspectratio,scale=0.6]{Camera_Perspective_v2.png}
\end{figure}

\par
The algorithm to generate a ray from the perspective camera is very simple.
By knowing the horizontal Field of View and vertical Field of View in radians, and with
$u$
and
$v$
as parameters, it is possible to calculate the direction of that ray.

\par
It starts to calculate the distance to go through the right vector and the up vector of the camera.
That can be done with the arctangent of each Field of View of the camera (horizontal and vertical) and multiplying it with
$u - 0.5$
in the right vector and with
$0.5 - v$
in the up vector.
This makes sure that we go through every pixel, starting with the pixel from the top left corner to the bottom right corner of the image plane.
Then the destination point of the ray is just the sum:\\
$destinationPoint = cameraPosition + cameraDirection + rightVector + upVector$
, and so its direction is just:\\
$rayDirection = destinationPoint - cameraPosition$
.
The origin of the ray is the position of the camera, because in a perspective camera, all rays come from the same point: the point where the camera is located.

\begin{lstlisting}[caption={Algorithm of generateRay in Perspective Camera}, captionpos=b, label=Perspective]
float rightFactor = arctan(hFov * (u - 0.5)) + deviationU
glm::vec3 rightVector = right * rightFactor
float upFactor = arctan(vFov * (0.5 - v)) + deviationV
glm::vec3 upVector = up * upFactor
glm::vec3 destinationPoint = cameraPosition + cameraDirection + rightVector + upVector
glm::vec3 rayDirection = destinationPoint - cameraPosition
Ray ray = Ray(glm::normalize(rayDirection), cameraPosition)
return ray
\end{lstlisting}


\subsubsection{Orthographic Camera}

\par
In this projection mode, an object's size in the rendered image stays constant regardless of its distance from the camera.
This can be useful for rendering 2D scenes and UI elements, amongst other things.

\par
The orthographic camera removes the sense of perspective by drawing the image plane without simulating the depth of the objects, making all the objects looking flat.
This is achieved by inverting the logic of the perspective camera.
Instead of calculating the direction and always having the same origin, in the orthographic camera, the direction is always the same for all rays, and the origin of the ray varies.

\par
It starts to calculate the distance to go through the right vector and up vector of the camera, like the perspective camera.
But, instead of using the Field of View, we now use the
$sizeH$
and
$sizeV$
, which are the horizontal and vertical sizes of the image plane.
Then, to make sure that we go through all pixels from top left pixel to the bottom right, we need to use the
$u$
and
$v$
values from the parameters.
Then, the origin of the ray is:
$rayOrigin = cameraPosition + rightVector + upVector$
.

\begin{lstlisting}[caption={Algorithm of generateRay in Orthographic Camera}, captionpos=b, label=Orthographic]
float rightFactor = (u - 0.5) * sizeH
glm::vec3 rightVector = right * rightFactor + right * deviationU
float upFactor = (0.5 - v) * sizeV
glm::vec3 upVector = up * upFactor + up * deviationV
glm::vec3 rayOrigin = cameraPosition + rightVector + upVector
Ray ray = Ray (cameraDirection, rayOrigin)
return ray
\end{lstlisting}

\begin{figure}[H]
	\centering
	\caption{Orthographic camera with sizeH and sizeV (\cite{Camera_Perspective_Orthographic}).}
	\label{Perspective_Camera.}
	\includegraphics[keepaspectratio,scale=0.6]{Camera_Orthographic_v2.png}
\end{figure}


\subsection{Object Loaders}

\par
Last, but not least, the library allows the programmer to develop his / her own Object Loaders.
An Object Loader is, as the name implies, a class that allows the ray tracer to import meshes from different Model file formats.
There are many different types of file formats for storing meshes:

\begin{itemize}
	\item 3ds
	\item FBX
	\item Wavefront .obj file
	\item COLLADA
	\item SketchUp
	\item AutoCAD DXF
\end{itemize}

\par
All these file formats allows the user to store the positions of many triangles in a file, which together form the geometry of the scene.

\par
This library only provides one Object Loader that allows loading the scene geometry from Wavefront obj files, which was achieved using a third party library called "tinyobjloader".
This type of Model file format is a good choice because, besides being simple, it is open and has been adopted by many 3D graphics application vendors, like, 3ds Max and Blender.

\subsubsection{OBJ Loader}

\par
The class "OBJLoader" is a Rendering Component that allows the user to load 3D scenes from Wavefront obj files.

\par
The OBJ file format is a simple file data format that allows to store only the position of each vertex, the UV position of each texture coordinate vertex, each vertex normal, and the faces that make each polygon defined as a list of vertices, and texture vertices.
Vertices are stored in a counter-clockwise order by default, making explicit declaration of face normals unnecessary.

\begin{lstlisting}[caption={.OBJ file format}, captionpos=b, label=OBJ]
	# This is a comment

	# List of geometric vertices
	# x   y   z   w
	v 1.0 2.0 3.0 1.0
	v 5.0 6.0 7.0 1.0
	...
	
	# List of texture coordinates
	#  x   y   w
	vt 0.5 1.0 0.0
	vt 0.1 0.3 0.0
	...
	
	# List of vertices normals
	#  x   y   z
	vn 0.3 0.0 0.7
	vn 0.2 1.0 0.3
	...
	
	# List of polygonal face elements
	# v1/vt1/vn1	v2/vt2/vn2	v3/vt3/vn3
	f 03/01/01			04/02/03		05/03/04
	f 06/06/03			07/01/02		08/02/06
	...
\end{lstlisting}

\par
The Wavefront OBJ file format is described as shown in \ref{OBJ}, and is accompanied by another file, Material Template Library (MTL), that describes the visual aspects of the polygons.
It is this file that describes surface shading (material) properties of objects within one or more .OBJ files.
It allows to define multiple materials, with, ambient color, diffuse color, specular reflective color and specular refractive color.
And even allows to load texture maps stored in TGA files.
The listing \ref{MTL} shows an example of a description of a material.

\begin{lstlisting}[caption={.MTL file format}, captionpos=b, label=MTL]
# This is a comment

# Define a material named 'Colored'
newmtl Colored

# Ambient color
Ka 1.0 0.0 0.0	# red

# Diffuse color
Kd 0.0 1.0 0.0	# green

# Specular color
Ks 0.0 0.0 1.0	# blue

# Dissolved (transparency)
d 0.4
\end{lstlisting}